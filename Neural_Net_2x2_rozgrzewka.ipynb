{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff78f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f9fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy funkcje aktywacji fi (sigmoida)\n",
    "# fi' = fi*(1-fi) \n",
    "\n",
    "def sigmoid(x): return (1+np.exp(-x))**(-1) \n",
    "\n",
    "def deriv_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Odwrotna funkcja sigmoidalna - Czy można jej tutaj użyć?\n",
    "def inv_sigmoid(x): return np.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generujemy wagi\n",
    "def wygeneruj_wagi(wejscie, wyjscie):\n",
    "    # wyjściem jest liczba neuronów w następnej warstwie\n",
    "    wektor_wag = np.random.normal(0,1/math.sqrt(len(wejscie)),(1+len(wejscie))*wyjscie)\n",
    "    return np.reshape(wektor_wag, (wyjscie, 1+len(wejscie) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3f3320",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mwygeneruj_wagi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mwygeneruj_wagi\u001b[39m\u001b[34m(wejscie, wyjscie)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwygeneruj_wagi\u001b[39m(wejscie, wyjscie):\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# wyjściem jest liczba neuronów w następnej warstwie\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     wektor_wag = np.random.normal(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m/\u001b[43mmath\u001b[49m.sqrt(\u001b[38;5;28mlen\u001b[39m(wejscie)),(\u001b[32m1\u001b[39m+\u001b[38;5;28mlen\u001b[39m(wejscie))*wyjscie)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.reshape(wektor_wag, (wyjscie, \u001b[32m1\u001b[39m+\u001b[38;5;28mlen\u001b[39m(wejscie) )  )\n",
      "\u001b[31mNameError\u001b[39m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "wygeneruj_wagi((1,2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c348c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wejscie = wektor 784\n",
    "# Wyjscie = wektor 10\n",
    "\n",
    "# nerunony_na_warstwe: podajemy liczbę na warstwy głębokie\n",
    "def forward_prop(wejscie_dane, liczb_neuronow_wyjscie ):\n",
    "    \"\"\"Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "    Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "    Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "    Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "    Musimy też zapisywać stany x, net, a dla każdej warstwy (np. lista krotek)\"\"\"\n",
    "\n",
    "    stany_x_net_a = []\n",
    "    # Musimy przerobić X na wektor kolumnowy\n",
    "    X = np.vstack([1,wejscie_dane.reshape(-1,1)])\n",
    "    W = wygeneruj_wagi(wejscie_dane, liczb_neuronow_wyjscie)\n",
    "    net = W @ X\n",
    "    a = sigmoid(net)\n",
    "    stany_x_net_a.append((X,net,a))\n",
    "    return stany_x_net_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e885679",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = forward_prop(np.array([1,1,0]), liczb_neuronow_wyjscie=2)[0][2]\n",
    "print(A1)\n",
    "\n",
    "A2 = forward_prop(A1, liczb_neuronow_wyjscie=2)[0][2]\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy propagację wstecz\n",
    "\n",
    "# wyjscie oczekiwane - to co chcielibyśmy uzyskać\n",
    "# wyjscie dane - nasz wynik z forward_prop\n",
    "\n",
    "# Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "def back_prop(wyjscie_oczekiwane, wyjscie_dane, wagi_warstwy, stala_uczenia = 0.01 ):\n",
    "\n",
    "    # Ostatnia warstwa\n",
    "    a_ost = \n",
    "    dL_a_wyjscia = wyjscie_dane - wyjscie_oczekiwane\n",
    "\n",
    "    # mamy pochodną dL/da * fi(net)\n",
    "    delta = dL_a_wyjscia * deriv_sigmoid( inv_sigmoid(wyjscie_dane) )\n",
    "    dL_dW = delta * wyjscie_dane.T\n",
    "\n",
    "    # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "    dL_dX = wagi_warstwy.T @ delta\n",
    "\n",
    "    dL_da_nizsze = dL_dX[1:]\n",
    "\n",
    "    # Aktualizujemy wagi\n",
    "    W_nowe = wagi_warstwy - stala_uczenia*dL_dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaecbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.1,-0.2,0.3], [-0.4,0.5,-0.6]])\n",
    "B = np.array([1,1,0])\n",
    "C= B.reshape(-1,1)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b666cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([1,A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f793c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejsc, liczba_neuronow, funkcja_aktywacji='sigmoid'):\n",
    "        self.liczba_wejsc = liczba_wejsc\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "        \n",
    "        # Inicjalizacja wag - Xavier/Glorot initialization\n",
    "        self.wagi = np.random.randn(liczba_neuronow, liczba_wejsc + 1) * np.sqrt(2.0 / (liczba_wejsc + liczba_neuronow))\n",
    "        \n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.wejscie = None\n",
    "        self.net = None\n",
    "        self.wyjscie = None\n",
    "        \n",
    "    def aktywacja(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # Clip dla stabilności numerycznej\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            return x  # Linear\n",
    "    \n",
    "    def pochodna_aktywacji(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            s = self.aktywacja(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return np.ones_like(x)  # Linear\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Dodajemy bias\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        X_z_biasem = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        \n",
    "        self.wejscie = X_z_biasem\n",
    "        self.net = X_z_biasem @ self.wagi.T\n",
    "        self.wyjscie = self.aktywacja(self.net)\n",
    "        \n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, delta_nastepna, stala_uczenia):\n",
    "        # delta_nastepna: błąd z następnej warstwy\n",
    "        delta = delta_nastepna * self.pochodna_aktywacji(self.net)\n",
    "        \n",
    "        # Oblicz gradienty wag\n",
    "        dW = delta.T @ self.wejscie / self.wejscie.shape[0]\n",
    "        \n",
    "        # Aktualizuj wagi\n",
    "        self.wagi -= stala_uczenia * dW\n",
    "        \n",
    "        # Oblicz błąd do przekazania do poprzedniej warstwy (bez biasu)\n",
    "        delta_prev = delta @ self.wagi[:, 1:]\n",
    "        \n",
    "        return delta_prev\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self, architektura, funkcje_aktywacji=None):\n",
    "        self.warstwy = []\n",
    "        \n",
    "        if funkcje_aktywacji is None:\n",
    "            funkcje_aktywacji = ['sigmoid'] * (len(architektura) - 1)\n",
    "        \n",
    "        for i in range(len(architektura) - 1):\n",
    "            warstwa = Warstwa(architektura[i], architektura[i+1], funkcje_aktywacji[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        wyjscie = X\n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward(wyjscie)\n",
    "        return wyjscie\n",
    "    \n",
    "    def backward(self, y, stala_uczenia):\n",
    "        # Oblicz błąd na wyjściu\n",
    "        wyjscie_ost = self.warstwy[-1].wyjscie\n",
    "        delta = wyjscie_ost - y\n",
    "        \n",
    "        # Propagacja wsteczna przez wszystkie warstwy\n",
    "        for i in range(len(self.warstwy) - 1, -1, -1):\n",
    "            delta = self.warstwy[i].backward(delta, stala_uczenia)\n",
    "    \n",
    "    def fit(self, X, y, epoki=100, stala_uczenia=0.1, rozmiar_batcha=32, verbose=True):\n",
    "        n_przykladow = X.shape[0]\n",
    "        historia_straty = []\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Tasowanie danych\n",
    "            indeksy = np.random.permutation(n_przykladow)\n",
    "            X_tasowane = X[indeksy]\n",
    "            y_tasowane = y[indeksy]\n",
    "            \n",
    "            strata_epoki = 0\n",
    "            \n",
    "            for i in range(0, n_przykladow, rozmiar_batcha):\n",
    "                koniec = min(i + rozmiar_batcha, n_przykladow)\n",
    "                X_batch = X_tasowane[i:koniec]\n",
    "                y_batch = y_tasowane[i:koniec]\n",
    "                \n",
    "                # Forward propagation\n",
    "                wyjscie = self.forward(X_batch)\n",
    "                \n",
    "                # Oblicz stratę (mean squared error)\n",
    "                strata = np.mean((wyjscie - y_batch) ** 2)\n",
    "                strata_epoki += strata * (koniec - i)\n",
    "                \n",
    "                # Backward propagation\n",
    "                self.backward(y_batch, stala_uczenia)\n",
    "            \n",
    "            strata_epoki /= n_przykladow\n",
    "            historia_straty.append(strata_epoki)\n",
    "            \n",
    "            if verbose and epoka % 10 == 0:\n",
    "                print(f\"Epoka {epoka}, Strata: {strata_epoki:.4f}\")\n",
    "        \n",
    "        return historia_straty\n",
    "    \n",
    "    def predict(self, X):\n",
    "        wyjscie = self.forward(X)\n",
    "        return np.argmax(wyjscie, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        przewidywania = self.predict(X)\n",
    "        prawdziwe_etykiety = np.argmax(y, axis=1)\n",
    "        return np.mean(przewidywania == prawdziwe_etykiety)\n",
    "\n",
    "# Funkcje do ładowania danych MNIST\n",
    "def load_mnist():\n",
    "    \"\"\"Ładuje dane MNIST\"\"\"\n",
    "    try:\n",
    "        with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik mnist.pkl.gz nie został znaleziony.\")\n",
    "        print(\"Proszę pobrać dane MNIST i umieścić je w tym samym katalogu.\")\n",
    "        return None, None, None\n",
    "\n",
    "def przygotuj_dane(train_set, test_set):\n",
    "    \"\"\"Przygotowuje dane do treningu\"\"\"\n",
    "    X_train, y_train = train_set\n",
    "    X_test, y_test = test_set\n",
    "    \n",
    "    # Normalizacja pikseli do zakresu [0, 1]\n",
    "    X_train = X_train / 255.0\n",
    "    X_test = X_test / 255.0\n",
    "    \n",
    "    # Konwersja etykiet na one-hot encoding\n",
    "    y_train_onehot = np.eye(10)[y_train]\n",
    "    y_test_onehot = np.eye(10)[y_test]\n",
    "    \n",
    "    return X_train, X_test, y_train_onehot, y_test_onehot\n",
    "\n",
    "# Eksperymenty z różnymi architekturami\n",
    "def eksperymentuj():\n",
    "    train_set, valid_set, test_set = load_mnist()\n",
    "    if train_set is None:\n",
    "        return\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = przygotuj_dane(train_set, test_set)\n",
    "    \n",
    "    # Różne architektury do przetestowania\n",
    "    architektury = [\n",
    "        [784, 128, 10],           # 1 warstwa ukryta, 128 neuronów\n",
    "        [784, 256, 10],           # 1 warstwa ukryta, 256 neuronów  \n",
    "        [784, 64, 64, 10],        # 2 warstwy ukryte, 64 neurony każda\n",
    "        [784, 128, 64, 10],       # 2 warstwy ukryte, różne rozmiary\n",
    "        [784, 512, 256, 10],      # 2 warstwy ukryte, większe\n",
    "    ]\n",
    "    \n",
    "    for i, architektura in enumerate(architektury):\n",
    "        print(f\"\\n--- Eksperyment {i+1}: {architektura} ---\")\n",
    "        \n",
    "        # Użyj ReLU dla warstw ukrytych, softmax nie jest potrzebny z MSE\n",
    "        funkcje_aktywacji = ['relu'] * (len(architektura) - 2) + ['sigmoid']\n",
    "        \n",
    "        siec = SiecNeuronowa(architektura, funkcje_aktywacji)\n",
    "        \n",
    "        print(\"Trenowanie...\")\n",
    "        historia = siec.fit(X_train, y_train, epoki=50, stala_uczenia=0.1, rozmiar_batcha=64)\n",
    "        \n",
    "        dokladnosc_train = siec.accuracy(X_train, y_train)\n",
    "        dokladnosc_test = siec.accuracy(X_test, y_test)\n",
    "        \n",
    "        print(f\"Dokładność na zbiorze treningowym: {dokladnosc_train:.4f}\")\n",
    "        print(f\"Dokładność na zbiorze testowym: {dokladnosc_test:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eksperymentuj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23466bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
