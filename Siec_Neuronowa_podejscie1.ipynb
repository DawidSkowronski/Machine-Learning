{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c21de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import load_data_wrapper\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f6d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_treningowe, dane_walidacyjne, dane_testowe = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db907ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musimy unormować dane\n",
    "def normuj_dane(dane):\n",
    "    unormowane = []\n",
    "    for  x, y in dane:\n",
    "        x_norm = x/255\n",
    "        unormowane.append((x_norm,y))\n",
    "    return unormowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "dane_treningowe = normuj_dane(dane_treningowe) # normlaizujemy dane\n",
    "dane_walidacyjne = normuj_dane(dane_walidacyjne)\n",
    "dane_testowe = normuj_dane(dane_testowe) # normalizujemy dane\n",
    "\n",
    "print(len(dane_treningowe)) \n",
    "print(len(dane_walidacyjne))\n",
    "print(len(dane_testowe)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ae1d0",
   "metadata": {},
   "source": [
    "#### Definiujemy byty\n",
    "\n",
    "* Warstwa, metody:\n",
    "1. forward propagation\n",
    "2. back propagation\n",
    "\n",
    " Warstwa powinna mieć atrybuty:\n",
    "- wagi\n",
    "- dane_na_wejsciu (zapamietać)\n",
    "- dane_na_wyjsciu (przekazujemy)\n",
    "- liczba neuronow\n",
    "- funkcja aktywacji\n",
    "- net (zapamiętać)\n",
    "\n",
    "Zapamiętać np. w liście\n",
    "\n",
    "* Sieć (składa sie z wielu Warstw), metody\n",
    "1. forward prop - wykonujemy na każdej z warstw\n",
    "2. backward prop - wykonujemy na każdej z warstw\n",
    "3. fit\n",
    "4. predict\n",
    "5. dokladnosc\n",
    "\n",
    "* Czy dobre byłoby także znormalizować dane, aby przyjmowały wartości z przedziału [0,1]?\n",
    " W ten sposób zminimalizujemy ryzyko liczenia gradientów o bardzo dużych wartościach. Pytanie czy wtedy trzeba generować dane z rozkladu $\\mathcal{N}(0,1/\\sqrt{n})$, n - liczba danych na wejściu\n",
    " Czy wystarczy $\\mathcal{N}(0,1)$\n",
    "\n",
    " * Czy aktualizację wag lepiej jest robić w osobnej funkcji w klasie Warstwa, czy podczas back_prop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee0f8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wygeneruj_wagi(wymiar_wejscie, wymiar_wyjscie):\n",
    "    # wyjściem jest liczba neuronów w następnej warstwie\n",
    "    wektor_wag = np.random.normal(0,1/math.sqrt(wymiar_wejscie),(1+wymiar_wejscie)*wymiar_wyjscie)\n",
    "    return np.reshape(wektor_wag, (wymiar_wyjscie, 1+wymiar_wejscie )  )\n",
    "\n",
    "# Definiujemy funkcje aktywacji fi (sigmoid)\n",
    "# fi' = fi*(1-fi) \n",
    "def sigmoid(x): return (1+np.exp(-x))**(-1) \n",
    "\n",
    "def deriv_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10]):\n",
    "        # definiujemy listę obiektów warstwy\n",
    "        self.warstwy = list()\n",
    "        # Może każdej warstwy definiujemy funkcję aktywacji? (W ten sposób można wykorzystać różne funkcje)\n",
    "\n",
    "        for i in range(len(wymiary)-1):\n",
    "            warstwa = Warstwa(wymiary[i+1], wymiary[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        wyjscie = X\n",
    "        \n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward_prop(wyjscie)\n",
    "        return wyjscie\n",
    "            \n",
    "\n",
    "    def backward_propagation(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def dokladnosc(self):\n",
    "        pass\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_neuronow, dane_wejscie):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.dane_wejscie = dane_wejscie\n",
    "        self.liczba_wejscie = len(self.dane_wejscie)\n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.dane_wejscie, self.liczba_neuronow)\n",
    "\n",
    "        # Rzeczy do zapamiętania\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "\n",
    "        # Musimy przerobić X na wektor kolumnowy\n",
    "        self.X = np.vstack([1, dane_wejscie.reshape(-1,1)])\n",
    "        self.net = self.wagi @ self.X\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscia_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "        dL_a = self.wyjscia_forward_prop - wyjscie_oczekiwane\n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        dL_dW = delta @ self.X.T\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45c690fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,0]).reshape(2,1)\n",
    "y = np.array([0,1]).reshape(2,1)\n",
    "W1 = np.array([[-0.1, -0.2, 0.1],\n",
    "               [-0.4, 0.7, -0.6]])\n",
    "W2 = np.array([[-0.15, -0.25, 0.15],\n",
    "               [-0.45, 0.75, -0.65]])\n",
    "X1 = np.vstack((1,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45c68e",
   "metadata": {},
   "source": [
    "#### Lepiej będzie chyba podawać w `__init__` Warstwa liczbę danych na wejściu i liczbę neuronów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14ff6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10]):\n",
    "        # definiujemy listę obiektów warstwy\n",
    "        self.warstwy = list()\n",
    "        # Może każdej warstwy definiujemy funkcję aktywacji? (W ten sposób można wykorzystać różne funkcje)\n",
    "\n",
    "        for i in range(len(wymiary)-1):\n",
    "            warstwa = Warstwa(wymiary[i+1], wymiary[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        wyjscie = X\n",
    "        \n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward_prop(wyjscie)\n",
    "            print(\"wyjscie z warstwy\")\n",
    "            print(wyjscie)\n",
    "        return wyjscie\n",
    "            \n",
    "\n",
    "    def backward_propagation(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def dokladnosc(self):\n",
    "        pass\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejscie, liczba_neuronow):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.liczba_wejscie = liczba_wejscie\n",
    "        \n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.liczba_wejscie, self.liczba_neuronow)\n",
    "\n",
    "        # Rzeczy do zapamiętania\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "\n",
    "        # Musimy przerobić X na wektor kolumnowy\n",
    "        self.X = np.vstack([1, dane_wejscie.reshape(-1,1)])\n",
    "        self.net = self.wagi @ self.X\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscia_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "\n",
    "        # Ten fragment chyba nie jest potrzeby\n",
    "        dL_a = wyjscie_oczekiwane\n",
    "        #dL_a = self.wyjscia_forward_prop - wyjscie_oczekiwane\n",
    "        #print(\"dL_a\")\n",
    "        #print(dL_a)\n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        print(\"delta\")\n",
    "        print(delta)\n",
    "\n",
    "        dL_dW = delta @ self.X.T\n",
    "        print(\"dL_dW\")\n",
    "        print(dL_dW)\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[1]\n",
      "[[0.42555748]\n",
      " [0.57444252]]\n",
      "a[2]\n",
      "[[0.45754671]\n",
      " [0.37654958]]\n",
      "dL_da2\n",
      "[[ 0.45754671]\n",
      " [-0.62345042]]\n",
      "\n",
      "delta\n",
      "[[ 0.11356205]\n",
      " [-0.14636122]]\n",
      "dL_dW\n",
      "[[ 0.11356205  0.04832718  0.06523487]\n",
      " [-0.14636122 -0.06228511 -0.08407611]]\n",
      "dL_da1\n",
      "[[-0.13816143]\n",
      " [ 0.1121691 ]]\n",
      "nowe_wagi dla warstwy 2\n",
      "[[-0.16135621 -0.25483272  0.14347651]\n",
      " [-0.43536388  0.75622851 -0.64159239]]\n",
      "Obliczenia dla warstwy niżej\n",
      "\n",
      "delta\n",
      "[[-0.03377471]\n",
      " [ 0.02742067]]\n",
      "dL_dW\n",
      "[[-0.03377471 -0.03377471  0.        ]\n",
      " [ 0.02742067  0.02742067  0.        ]]\n",
      "dL_da0\n",
      "[[ 0.02594941]\n",
      " [-0.01982987]]\n",
      "nowe wagi dla warstwy 1\n",
      "[[-0.09662253 -0.19662253  0.1       ]\n",
      " [-0.40274207  0.69725793 -0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,0]).reshape(2,1)\n",
    "y = np.array([0,1]).reshape(2,1)\n",
    "W1 = np.array([[-0.1, -0.2, 0.1],\n",
    "               [-0.4, 0.7, -0.6]])\n",
    "W2 = np.array([[-0.15, -0.25, 0.15],\n",
    "               [-0.45, 0.75, -0.65]])\n",
    "X1 = np.vstack((1,x))\n",
    "\n",
    "warstwa1 = Warstwa(2,2)\n",
    "warstwa1.wagi = W1\n",
    "print(\"a[1]\")\n",
    "print(warstwa1.forward_prop(x)) # dostajemy a[1] z przykładu ML_06\n",
    "a1 = warstwa1.forward_prop(x)\n",
    "\n",
    "\n",
    "warstwa2 = Warstwa(2,2)\n",
    "warstwa2.wagi = W2\n",
    "#print(warstwa2.wagi)\n",
    "a2 = warstwa2.forward_prop(a1)\n",
    "print(\"a[2]\")\n",
    "print(a2)\n",
    "\n",
    "# Czyli forward_prop działa\n",
    "# Sprawdźmy back_prop\n",
    "\n",
    "#w back_prop podajemy dL_da[k], otrzymując w ten sposób dL_da[k-1] z niższej warstwy (w międzyczasie aktualizujemy wagi)\n",
    "dL_da2 = a2 - y\n",
    "print(\"dL_da2\")\n",
    "print(dL_da2)\n",
    "print()\n",
    "\n",
    "dL_da1 = warstwa2.back_prop(dL_da2)\n",
    "print(\"dL_da1\")\n",
    "print(dL_da1)\n",
    "warstwa2.lista_net #mamy net1\n",
    "\n",
    "print(\"nowe_wagi dla warstwy 2\")\n",
    "print(warstwa2.wagi)\n",
    "############################## Kolejna warstwa ##########################\n",
    "\n",
    "print(\"Obliczenia dla warstwy niżej\")\n",
    "print()\n",
    "\n",
    "dL_da0 = warstwa1.back_prop(dL_da1)\n",
    "print(\"dL_da0\")\n",
    "print(dL_da0)\n",
    "\n",
    "print(\"nowe wagi dla warstwy 1\")\n",
    "print(warstwa1.wagi)\n",
    "# Zwraca poprawne wyniki\n",
    "# Czyli forward i back prop są dobrze zdefiniowane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39961c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
