{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c21de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_loader import load_data_wrapper\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f6d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_treningowe, dane_walidacyjne, dane_testowe = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "dane_treningowe = list(dane_treningowe)\n",
    "dane_walidacyjne = list(dane_walidacyjne)\n",
    "dane_testowe = list(dane_testowe)\n",
    "\n",
    "print(len(dane_treningowe))\n",
    "print(len(dane_walidacyjne))\n",
    "print(len(dane_testowe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ae1d0",
   "metadata": {},
   "source": [
    "#### Definiujemy byty\n",
    "\n",
    "* Warstwa, metody:\n",
    "1. forward propagation\n",
    "2. back propagation\n",
    "\n",
    " Warstwa powinna mieć atrybuty:\n",
    "- wagi\n",
    "- dane_na_wejsciu (zapamietać)\n",
    "- dane_na_wyjsciu (przekazujemy)\n",
    "- liczba neuronow\n",
    "- funkcja aktywacji\n",
    "- net (zapamiętać)\n",
    "\n",
    "Zapamiętać np. w liście\n",
    "\n",
    "* Sieć (składa sie z wielu Warstw), metody\n",
    "1. fit\n",
    "2. predict\n",
    "3. dokladnosc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0f8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wygeneruj_wagi(wymiar_wejscie, wymiar_wyjscie):\n",
    "    # wyjściem jest liczba neuronów w następnej warstwie\n",
    "    wektor_wag = np.random.normal(0,1/math.sqrt(wymiar_wejscie),(1+wymiar_wejscie)*wymiar_wyjscie)\n",
    "    return np.reshape(wektor_wag, (wymiar_wyjscie, 1+wymiar_wejscie )  )\n",
    "\n",
    "# Definiujemy funkcje aktywacji fi (sigmoid)\n",
    "# fi' = fi*(1-fi) \n",
    "def sigmoid(x): return (1+np.exp(-x))**(-1) \n",
    "\n",
    "def deriv_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10], epoki = 10):\n",
    "        self.warstwa = Warstwa()\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        pass\n",
    "\n",
    "    def backward_propagation(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def dokladnosc(self):\n",
    "        pass\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_neuronow, dane_wejscie):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.dane_wejscie = dane_wejscie\n",
    "        self.liczba_wejscie = len(self.dane_wejscie)\n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.liczba_neuronow)\n",
    "\n",
    "        # Rzeczy do zapamiętania\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "\n",
    "        # Musimy przerobić X na wektor kolumnowy\n",
    "        self.X = np.vstack([1, dane_wejscie.reshape(-1,1)])\n",
    "        self.net = self.wagi @ self.X\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscia_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "        dL_a = self.wyjscia_forward_prop - wyjscie_oczekiwane\n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        dL_dW = delta @ self.X.T\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
