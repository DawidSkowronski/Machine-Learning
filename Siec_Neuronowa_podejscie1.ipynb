{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dane_treningowe, dane_walidacyjne, dane_testowe = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a520be",
   "metadata": {},
   "source": [
    "W danych treningowych y to jest liczba, natomiast w danych testowych y to wektor długości 10\n",
    "\n",
    "Dlatego zmieniamy to w pliku `mnist_loader.py` aby w obu przypadkach mieć wektor długości 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db907ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091c07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#dane_treningowe = normuj_dane(dane_treningowe) # normalizujemy dane\n",
    "#dane_walidacyjne = normuj_dane(dane_walidacyjne)\n",
    "#dane_testowe = normuj_dane(dane_testowe) # normalizujemy dane\n",
    "\n",
    "dane_treningowe = list(dane_treningowe)\n",
    "dane_walidacyjne = list(dane_walidacyjne)\n",
    "dane_testowe = list(dane_testowe)\n",
    "\n",
    "print(len(dane_treningowe)) \n",
    "print(len(dane_walidacyjne))\n",
    "print(len(dane_testowe)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c3544b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dane_treningowe[1][0].shape) # wymiar danych\n",
    "print(dane_treningowe[1][1].shape) # wymiar etykiety\n",
    "# Czyli mamy wektory kolumnowe\n",
    "\n",
    "# Trzeba je przekonwertować na wierszowe\n",
    "\n",
    "# Mozna to zmienić w load_data_wrapper()\n",
    "# Można też robić to w sieci (na razie ten wariant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ae1d0",
   "metadata": {},
   "source": [
    "#### Definiujemy byty\n",
    "\n",
    "* Warstwa, metody:\n",
    "1. forward propagation\n",
    "2. back propagation\n",
    "\n",
    " Warstwa powinna mieć atrybuty:\n",
    "- wagi\n",
    "- dane_na_wejsciu (zapamietać)\n",
    "- dane_na_wyjsciu (przekazujemy)\n",
    "- liczba neuronow\n",
    "- funkcja aktywacji\n",
    "- net (zapamiętać)\n",
    "\n",
    "Zapamiętać np. w liście\n",
    "\n",
    "* Sieć (składa sie z wielu Warstw), metody\n",
    "1. forward prop - wykonujemy na każdej z warstw\n",
    "2. backward prop - wykonujemy na każdej z warstw\n",
    "3. fit\n",
    "4. predict\n",
    "5. dokladnosc\n",
    "\n",
    "* Czy dobre byłoby także znormalizować dane, aby przyjmowały wartości z przedziału [0,1]?\n",
    " W ten sposób zminimalizujemy ryzyko liczenia gradientów o bardzo dużych wartościach. Pytanie czy wtedy trzeba generować dane z rozkladu $\\mathcal{N}(0,1/\\sqrt{n})$, n - liczba danych na wejściu\n",
    " Czy wystarczy $\\mathcal{N}(0,1)$\n",
    "\n",
    " * Czy aktualizację wag lepiej jest robić w osobnej funkcji w klasie Warstwa, czy podczas back_prop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0f8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wygeneruj_wagi(wymiar_wejscie, wymiar_wyjscie):\n",
    "    # wyjściem jest liczba neuronów w następnej warstwie\n",
    "    wektor_wag = np.random.normal(0,1/math.sqrt(wymiar_wejscie),(1+wymiar_wejscie)*wymiar_wyjscie)\n",
    "    return np.reshape(wektor_wag, (wymiar_wyjscie, 1+wymiar_wejscie )  )\n",
    "\n",
    "# Definiujemy funkcje aktywacji fi (sigmoid)\n",
    "# fi' = fi*(1-fi) \n",
    "def sigmoid(x): return (1+np.exp(-x))**(-1) \n",
    "\n",
    "def deriv_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa28b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10]):\n",
    "        # definiujemy listę obiektów warstwy\n",
    "        self.warstwy = list()\n",
    "        # Może każdej warstwy definiujemy funkcję aktywacji? (W ten sposób można wykorzystać różne funkcje)\n",
    "\n",
    "        for i in range(len(wymiary)-1):\n",
    "            warstwa = Warstwa(wymiary[i+1], wymiary[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        wyjscie = X\n",
    "        \n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward_prop(wyjscie)\n",
    "        return wyjscie\n",
    "            \n",
    "\n",
    "    def backward_propagation(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def dokladnosc(self):\n",
    "        pass\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_neuronow, dane_wejscie):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.dane_wejscie = dane_wejscie\n",
    "        self.liczba_wejscie = len(self.dane_wejscie)\n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.dane_wejscie, self.liczba_neuronow)\n",
    "\n",
    "        # Rzeczy do zapamiętania\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "\n",
    "        # Musimy przerobić X na wektor kolumnowy\n",
    "        self.X = np.vstack([1, dane_wejscie.reshape(-1,1)])\n",
    "        self.net = self.wagi @ self.X\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscia_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "        dL_a = self.wyjscia_forward_prop - wyjscie_oczekiwane\n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        dL_dW = delta @ self.X.T\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c690fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,0]).reshape(2,1)\n",
    "y = np.array([0,1]).reshape(2,1)\n",
    "W1 = np.array([[-0.1, -0.2, 0.1],\n",
    "               [-0.4, 0.7, -0.6]])\n",
    "W2 = np.array([[-0.15, -0.25, 0.15],\n",
    "               [-0.45, 0.75, -0.65]])\n",
    "X1 = np.vstack((1,x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b45c68e",
   "metadata": {},
   "source": [
    "#### Lepiej będzie chyba podawać w `__init__` Warstwa liczbę danych na wejściu i liczbę neuronów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejscie, liczba_neuronow):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.liczba_wejscie = liczba_wejscie\n",
    "        \n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.liczba_wejscie, self.liczba_neuronow)\n",
    "        print(f\"wagi: {self.wagi.shape}\")\n",
    "\n",
    "        # Rzeczy do zapamiętania,\n",
    "        #  Może wystarczy zapamiętać tylko ostatnie wartosci\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "        self.wyjscie_forward_prop = None\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "\n",
    "        # Musimy przerobić X na wektor kolumnowy\n",
    "        self.X = np.vstack([1, dane_wejscie.reshape(-1,1)])\n",
    "        self.net = self.wagi @ self.X\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscie_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane, czy_warstwa_wyjsciowa = False):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "\n",
    "        # Ten fragment chyba nie jest potrzeby\n",
    "        dL_a = wyjscie_oczekiwane\n",
    "        #dL_a = self.wyjscia_forward_prop - wyjscie_oczekiwane\n",
    "        #print(\"dL_a\")\n",
    "        #print(dL_a)\n",
    "\n",
    "        if czy_warstwa_wyjsciowa == True:\n",
    "            dL_a = wyjscie_oczekiwane * self.pochodna_fun_aktywacji(self.net)\n",
    "        else:\n",
    "            wagi_bez_biasu = self.wagi[:, 1:] #pomijamy wagę dla biasu\n",
    "            delta = \n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        print(\"delta\")\n",
    "        print(delta)\n",
    "\n",
    "        dL_dW = delta @ self.X.T\n",
    "        print(\"dL_dW\")\n",
    "        print(dL_dW)\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "    \n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10]):\n",
    "        # definiujemy listę obiektów warstwy\n",
    "        self.warstwy = list()\n",
    "        # Może każdej warstwy definiujemy funkcję aktywacji? (W ten sposób można wykorzystać różne funkcje)\n",
    "\n",
    "        for i in range(len(wymiary)-1):\n",
    "            warstwa = Warstwa(wymiary[i], wymiary[i+1]) # Liczba na wejście i na wyjście\n",
    "            self.warstwy.append(warstwa)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        wyjscie = X\n",
    "        \n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward_prop(wyjscie)\n",
    "            print(\"wyjscie z warstwy\")\n",
    "            print(wyjscie)\n",
    "        return wyjscie\n",
    "            \n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "\n",
    "        ostatnie_wyjscie = self.warstwy[-1].wyjscia_forward_prop\n",
    "\n",
    "        dL_da = ostatnie_wyjscie - y\n",
    "\n",
    "        for i in range(len(self.warstwy) - 1, -1, -1):\n",
    "            dL_da = self.warstwy[i].back_prop(dL_da)\n",
    "        # Pytanie czy musi tu być niższe dL_da, czy podmieniamy obecne?\n",
    "        return dL_da\n",
    "    \n",
    "    def krok_uczenia(self, X, y):\n",
    "        \"\"\"Definiujemy pętlę dla jednej epoki\"\"\"\n",
    "\n",
    "        # Forward prop\n",
    "        y_estymowany = self.forward_propagation(X)\n",
    "\n",
    "        # Strata\n",
    "        strata = np.mean((y_estymowany - y)**2)\n",
    "\n",
    "        # Back prop\n",
    "        self.backward_propagation(y)\n",
    "\n",
    "        return strata # zwracamy wartośc funkcji straty\n",
    "\n",
    "    def fit(self, X, y, epoki = 10):\n",
    "        \"\"\"Przeprowadzamy fit() dla wielu epok\"\"\"\n",
    "\n",
    "        for epoka in range(epoki):\n",
    "\n",
    "            strata_w_epoce = 0\n",
    "            \n",
    "            # W każdej epoce \"mieszamy\" dane\n",
    "            # W tym celu \"mieszamy\" indeksy\n",
    "            indeksy = np.random.permutation(len(X))\n",
    "            X_tasowane = X[indeksy]\n",
    "            y_tasowane = y[indeksy]\n",
    "\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def dokladnosc(self):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adfae79",
   "metadata": {},
   "source": [
    "### Wersja pod mini-batch'e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "167e18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejscie, liczba_neuronow):\n",
    "        \"\"\"liczba neuronów - w warstwie (czyli liczba danych na wyjściu)\"\"\"\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "\n",
    "        self.liczba_wejscie = liczba_wejscie\n",
    "        \n",
    "        # Ustalamy stałą uczenia dla całej warstwy\n",
    "        self.stala_uczenia = 0.1\n",
    "\n",
    "        # Inicjalizacja wag\n",
    "        self.wagi = wygeneruj_wagi(self.liczba_wejscie, self.liczba_neuronow)\n",
    "\n",
    "        # Rzeczy do zapamiętania,\n",
    "        #  Może wystarczy zapamiętać tylko ostatnie wartosci\n",
    "        self.lista_dane_wejscie = list()\n",
    "        self.lista_net = list()\n",
    "    \n",
    "\n",
    "    def fun_aktywacji(self, x): return sigmoid(x)\n",
    "\n",
    "    def pochodna_fun_aktywacji(self, x): return deriv_sigmoid(x)\n",
    "\n",
    "    def forward_prop(self, dane_wejscie ):\n",
    "        \"\"\"Definiujemy propagację w przód.\n",
    "        Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "        Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "        Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "        Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "        Musimy też zapisywać stany x, net, a dla każdej warstwy (np. listy)\"\"\"\n",
    "        \n",
    "        rozmiar_batcha = dane_wejscie.shape[1]\n",
    "        biasy = np.ones((1, rozmiar_batcha))\n",
    "\n",
    "        self.X = np.vstack([biasy, dane_wejscie]) # Dodajemy biasy\n",
    "\n",
    "        self.net = self.wagi @ self.X\n",
    "\n",
    "        dane_wyjscie = self.fun_aktywacji(self.net)\n",
    "\n",
    "        self.lista_net.append(self.net)\n",
    "        self.lista_dane_wejscie.append(self.X)\n",
    "\n",
    "        self.wyjscia_forward_prop = dane_wyjscie\n",
    "\n",
    "        return dane_wyjscie\n",
    "    \n",
    "    def back_prop(self, dL_a):\n",
    "        \"\"\"Definiujemy propagację wstecz.\n",
    "        W ostatniej warstwie liczymy funkcję straty, następnie pochodną dL/da.\"\"\"\n",
    "        \n",
    "        # Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "\n",
    "        # mamy pochodną dL/da * fi(net)\n",
    "        delta = dL_a * self.pochodna_fun_aktywacji( self.net )\n",
    "        dL_dW = delta @ self.X.T\n",
    "\n",
    "        # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "        dL_dX = self.wagi.T @ delta\n",
    "        \n",
    "        # Dane do przekazania warstwę niżej\n",
    "        dL_a = dL_dX[1:]\n",
    "\n",
    "        # Aktualizujemy wagi\n",
    "        self.wagi = self.wagi - self.stala_uczenia * dL_dW\n",
    "\n",
    "        return dL_a\n",
    "    \n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self, wymiary = [784, 128, 64, 10]):\n",
    "        # definiujemy listę obiektów warstwy\n",
    "        self.warstwy = list()\n",
    "        # Może każdej warstwy definiujemy funkcję aktywacji? (W ten sposób można wykorzystać różne funkcje)\n",
    "\n",
    "        for i in range(len(wymiary)-1):\n",
    "            warstwa = Warstwa(wymiary[i], wymiary[i+1]) # Liczba na wejście i na wyjście\n",
    "            self.warstwy.append(warstwa)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        wejscie = X\n",
    "        \n",
    "        for warstwa in self.warstwy:\n",
    "            wejscie = warstwa.forward_prop(wejscie)\n",
    "\n",
    "        return wejscie\n",
    "            \n",
    "\n",
    "    def backward_propagation(self, y):\n",
    "\n",
    "        ostatnie_wyjscie = self.warstwy[-1].wyjscia_forward_prop\n",
    "\n",
    "        dL_da = ostatnie_wyjscie - y\n",
    "\n",
    "        for i in range(len(self.warstwy) - 1, -1, -1):\n",
    "            dL_da = self.warstwy[i].back_prop(dL_da)\n",
    "        # Pytanie czy musi tu być niższe dL_da, czy podmieniamy obecne?\n",
    "        return dL_da\n",
    "    \n",
    "    def krok_uczenia(self, X, y):\n",
    "        \"\"\"Definiujemy kroki dla jednej epoki\"\"\"\n",
    "\n",
    "        # Forward prop\n",
    "        y_estymowany = self.forward_propagation(X)\n",
    "\n",
    "        # Strata\n",
    "        strata = np.mean((y_estymowany - y)**2)\n",
    "\n",
    "        # Back prop\n",
    "        self.backward_propagation(y)\n",
    "\n",
    "        return strata # zwracamy wartośc funkcji straty\n",
    "\n",
    "    def fit(self, dane_w_krotkach, epoki = 10, rozmiar_batcha = 32):\n",
    "        \"\"\"Przeprowadzamy fit() dla wielu epok\"\"\"\n",
    "\n",
    "        straty = list()\n",
    "\n",
    "        for epoka in range(epoki):\n",
    "\n",
    "            strata_w_epoce = 0\n",
    "            liczba_batchy = 0\n",
    "\n",
    "            # W każdej epoce \"mieszamy\" dane\n",
    "            np.random.shuffle(dane_w_krotkach)\n",
    "\n",
    "            # dzielimy zbiór danych na porcje\n",
    "            for i in range(0, len(dane_w_krotkach), rozmiar_batcha): \n",
    "                batch = dane_w_krotkach[i:i + rozmiar_batcha]\n",
    "\n",
    "                # Wyciągamy z krotki osobno X i y\n",
    "                X_batch = np.hstack([x for x, y in batch])\n",
    "                y_batch = np.hstack([y for x, y in batch])\n",
    "                \n",
    "                # Forwad prop\n",
    "                #y_estymowany = self.forward_propagation(X_batch)\n",
    "\n",
    "                # Strata\n",
    "                strata = self.krok_uczenia(X_batch, y_batch)\n",
    "                strata_w_epoce += strata\n",
    "                liczba_batchy +=1\n",
    "            \n",
    "            srednia_strata = strata_w_epoce/liczba_batchy\n",
    "            straty.append(srednia_strata)\n",
    "\n",
    "            # Back prop\n",
    "            #self.backward_propagation(y_batch)\n",
    "            \n",
    "        return straty\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def dokladnosc(self, dane):\n",
    "        \"\"\" Oblicza dokładność klasyfikacji\"\"\"\n",
    "        poprawne = 0\n",
    "        liczba_danych = len(dane)\n",
    "\n",
    "        for x, y in dane:\n",
    "\n",
    "            # Z y daną cyfrę\n",
    "            y_prawdziwe = np.argmax(y)\n",
    "\n",
    "            przewidywane = self.predict(x)\n",
    "            # Wybieramy z wektora argument o najwyższej wartości\n",
    "            y_przewidywane = np.argmax(przewidywane)\n",
    "\n",
    "            # Sprawdzamy czy sieć dopasowałą poprawnie etykietę\n",
    "            if y_prawdziwe == y_przewidywane:\n",
    "                poprawne +=1\n",
    "        # Zwracamy dokładność\n",
    "        return poprawne/liczba_danych\n",
    "            \n",
    "def trenuj_siec():\n",
    "    dane_treningowe, dane_walidacyjne, dane_testowe = mnist_loader.load_data_wrapper()\n",
    "    # Pomijamy dane walidacyjne\n",
    "    dane_treningowe = list(dane_treningowe)\n",
    "    dane_testowe = list(dane_testowe)\n",
    "\n",
    "    # Tworzymy sieć\n",
    "    siec = SiecNeuronowa([784,256,64,10])\n",
    "\n",
    "    print(\"Trening\")\n",
    "    historia = siec.fit(dane_treningowe, epoki= 20, rozmiar_batcha= 128)\n",
    "\n",
    "    dokladnosc_trening = siec.dokladnosc(dane_treningowe)\n",
    "    dokladnosc_test = siec.dokladnosc(dane_testowe)\n",
    "\n",
    "    print(f\"Dokładność na zbiorze treningowym: {dokladnosc_trening:.4f}\")\n",
    "    print(f\"Dokładność na zbiorze testowym: {dokladnosc_test:.4f}\")\n",
    "\n",
    "    return siec, historia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eeab689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening\n",
      "Dokładność na zbiorze treningowym: 0.0998\n",
      "Dokładność na zbiorze testowym: 0.1009\n"
     ]
    }
   ],
   "source": [
    "siec, historia = trenuj_siec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeea3e9",
   "metadata": {},
   "source": [
    "Mamy dobre wyniki dla:\n",
    "* warstwy = [784,128,64,10], epoki = 5, rozmiar batcha = 32, stała uczenia = 0.1\n",
    "* warstwy = [784,128,64,32,10], epoki = 20, rozmiar batcha = 128, stała uczenia = 0.1\n",
    "\n",
    "Ogólne wnioski:\n",
    "* [784,256,64,10] np. dla takiej sieci wyniki są słabe, po 10*, nawet dla 20 epok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a69ed1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoki')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(historia)\n",
    "plt.title(\"Wartość funkcji straty w epokach\")\n",
    "plt.xlabel(\"Epoki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2201dd0",
   "metadata": {},
   "source": [
    "### Empiryczne sprawdzenie, czy Warstwa jest poprawnie zaimplementowana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ec2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wagi: (2, 3)\n",
      "a[1]\n",
      "dane_wejscie.shape (2, 1)\n",
      "self.X.shape (3, 1)\n",
      "wagi (2, 3)\n",
      "self.X.shape (2, 1)\n",
      "dane_wyjscie (2, 1)\n",
      "[[0.42555748]\n",
      " [0.57444252]]\n",
      "dane_wejscie.shape (2, 1)\n",
      "self.X.shape (3, 1)\n",
      "wagi (2, 3)\n",
      "self.X.shape (2, 1)\n",
      "dane_wyjscie (2, 1)\n",
      "wagi: (2, 3)\n",
      "dane_wejscie.shape (2, 1)\n",
      "self.X.shape (3, 1)\n",
      "wagi (2, 3)\n",
      "self.X.shape (2, 1)\n",
      "dane_wyjscie (2, 1)\n",
      "a[2]\n",
      "[[0.45754671]\n",
      " [0.37654958]]\n",
      "dL_da2\n",
      "[[ 0.45754671]\n",
      " [-0.62345042]]\n",
      "\n",
      "delta\n",
      "[[ 0.11356205]\n",
      " [-0.14636122]]\n",
      "dL_dW\n",
      "[[ 0.11356205  0.04832718  0.06523487]\n",
      " [-0.14636122 -0.06228511 -0.08407611]]\n",
      "dL_da1\n",
      "[[-0.13816143]\n",
      " [ 0.1121691 ]]\n",
      "nowe_wagi dla warstwy 2\n",
      "[[-0.16135621 -0.25483272  0.14347651]\n",
      " [-0.43536388  0.75622851 -0.64159239]]\n",
      "Obliczenia dla warstwy niżej\n",
      "\n",
      "delta\n",
      "[[-0.03377471]\n",
      " [ 0.02742067]]\n",
      "dL_dW\n",
      "[[-0.03377471 -0.03377471  0.        ]\n",
      " [ 0.02742067  0.02742067  0.        ]]\n",
      "dL_da0\n",
      "[[ 0.02594941]\n",
      " [-0.01982987]]\n",
      "nowe wagi dla warstwy 1\n",
      "[[-0.09662253 -0.19662253  0.1       ]\n",
      " [-0.40274207  0.69725793 -0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,0]).reshape(2,1)\n",
    "y = np.array([0,1]).reshape(2,1)\n",
    "W1 = np.array([[-0.1, -0.2, 0.1],\n",
    "               [-0.4, 0.7, -0.6]])\n",
    "W2 = np.array([[-0.15, -0.25, 0.15],\n",
    "               [-0.45, 0.75, -0.65]])\n",
    "X1 = np.vstack((1,x))\n",
    "\n",
    "warstwa1 = Warstwa(2,2)\n",
    "warstwa1.wagi = W1\n",
    "print(\"a[1]\")\n",
    "print(warstwa1.forward_prop(x)) # dostajemy a[1] z przykładu ML_06\n",
    "a1 = warstwa1.forward_prop(x)\n",
    "\n",
    "\n",
    "warstwa2 = Warstwa(2,2)\n",
    "warstwa2.wagi = W2\n",
    "#print(warstwa2.wagi)\n",
    "a2 = warstwa2.forward_prop(a1)\n",
    "print(\"a[2]\")\n",
    "print(a2)\n",
    "\n",
    "# Czyli forward_prop działa\n",
    "# Sprawdźmy back_prop\n",
    "\n",
    "#w back_prop podajemy dL_da[k], otrzymując w ten sposób dL_da[k-1] z niższej warstwy (w międzyczasie aktualizujemy wagi)\n",
    "dL_da2 = a2 - y\n",
    "print(\"dL_da2\")\n",
    "print(dL_da2)\n",
    "print()\n",
    "\n",
    "dL_da1 = warstwa2.back_prop(dL_da2)\n",
    "print(\"dL_da1\")\n",
    "print(dL_da1)\n",
    "warstwa2.lista_net #mamy net1\n",
    "\n",
    "print(\"nowe_wagi dla warstwy 2\")\n",
    "print(warstwa2.wagi)\n",
    "############################## Kolejna warstwa ##########################\n",
    "\n",
    "print(\"Obliczenia dla warstwy niżej\")\n",
    "print()\n",
    "\n",
    "dL_da0 = warstwa1.back_prop(dL_da1)\n",
    "print(\"dL_da0\")\n",
    "print(dL_da0)\n",
    "\n",
    "print(\"nowe wagi dla warstwy 1\")\n",
    "print(warstwa1.wagi)\n",
    "# Zwraca poprawne wyniki\n",
    "# Czyli forward i back prop są dobrze zdefiniowane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588168ea",
   "metadata": {},
   "source": [
    "Możemy ulepszyć tę sieć dodając np:\n",
    "* Osobną funkcję aktywacji dla każdej warsty np ReLU\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3720cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39961c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wagi: (2, 3)\n",
      "wagi: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "siec = SiecNeuronowa(wymiary=[2,2,2])\n",
    "siec.warstwy[0] = warstwa1\n",
    "siec.warstwy[1] = warstwa2\n",
    "\n",
    "#siec.forward_propagation(x)\n",
    "#siec.backward_propagation(dL_da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377e5f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ebe4a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(1, 4)\n",
      "[[1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "A_kolumnowy = A.reshape(-1,1) # kolumnowy\n",
    "A_wierszowy = A.reshape(1,-1) \n",
    "print(A_kolumnowy.shape)\n",
    "print(A_wierszowy.shape)\n",
    "print(A_wierszowy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "768174ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.ones((4,1))\n",
    "print(b)\n",
    "polaczone = np.hstack([b,A_kolumnowy])\n",
    "polaczone.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8c09b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = list((4,3,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ee0386d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(lista):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010499cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
