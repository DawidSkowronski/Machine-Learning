{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34b40038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2035172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "data = pd.read_csv(\"mnist_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58995fb6",
   "metadata": {},
   "source": [
    "# Najlepsze podejście do tej pory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab03de",
   "metadata": {},
   "source": [
    "Założenia:\n",
    "* Warstwa powinna przyjmować dane wejściowe z poprzedniej warstwy i zwracać dane wyjściowe do następnej warstwy\n",
    "- Musimy też zapamiętać x, net, a dla każdej warstwy\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf32d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prostej sieci:\n",
      "Wejście: [1 1 0]\n",
      "Wyjście: [0.5974812  0.46051038]\n",
      "Oczekiwane: [1 0]\n",
      "Wagi po aktualizacji:\n",
      "Warstwa 0:\n",
      "[[-0.63093114 -0.14279365  0.06030256  0.86744447]\n",
      " [-0.35832295  0.16731602 -0.10701603 -0.03344419]]\n",
      "Warstwa 1:\n",
      "[[ 0.19973145  0.32543509  0.21003857]\n",
      " [-0.28068419  0.29849856  0.05342589]]\n",
      "\n",
      "==================================================\n",
      "Test na danych MNIST:\n",
      "Ładowanie danych MNIST...\n",
      "Rozpoczynam uczenie...\n",
      "Epoka 0, Loss: 0.4525\n",
      "Epoka 10, Loss: 0.1761\n",
      "Epoka 20, Loss: 0.1181\n",
      "Epoka 30, Loss: 0.0949\n",
      "Dokładność: 0.9400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []  # lista wag dla każdej warstwy\n",
    "        self.stany = []    # przechowuje x, net, a dla każdej warstwy\n",
    "    \n",
    "    def dodaj_warstwe(self, rozmiar_wejscia, rozmiar_wyjscia):\n",
    "        \"\"\"Dodaje warstwę z losowymi wagami\"\"\"\n",
    "        wagi = self.wygeneruj_wagi(rozmiar_wejscia, rozmiar_wyjscia)\n",
    "        self.warstwy.append(wagi)\n",
    "    \n",
    "    def wygeneruj_wagi(self, rozmiar_wejscia, rozmiar_wyjscia):\n",
    "        \"\"\"Generuje wagi z rozkładu N(0, 1/sqrt(rozmiar_wejscia) )\"\"\"\n",
    "        return np.random.normal(0, 1/math.sqrt(rozmiar_wejscia), \n",
    "                               (rozmiar_wyjscia, 1 + rozmiar_wejscia))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "    \n",
    "    def deriv_sigmoid(self, x):\n",
    "        # Pochodna funkcji aktywacji\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward_prop(self, wejscie_dane):\n",
    "        \"\"\"Propagacja w przód przez wszystkie warstwy\"\"\"\n",
    "        self.stany = []  # reset stanów\n",
    "        \n",
    "        a = wejscie_dane.reshape(-1, 1)  # wektor kolumnowy\n",
    "        \n",
    "        for i, W in enumerate(self.warstwy): # Tutaj zmienić na for W in self.warstwy\n",
    "            # Dodajemy bias\n",
    "            x = np.vstack([1, a])\n",
    "            \n",
    "            # Obliczamy net i a\n",
    "            net = W @ x\n",
    "            a = self.sigmoid(net)\n",
    "            \n",
    "            # Zapamiętujemy stan\n",
    "            self.stany.append({'x': x, 'net': net, 'a': a}) # Zwracamy list≤ę słowników. Może łatwiej byłoby liste krotek\n",
    "        \n",
    "        return a.flatten()  # zwracamy jako wektor\n",
    "    \n",
    "    def back_prop(self, wyjscie_oczekiwane, stala_uczenia=0.01):\n",
    "        \"\"\"Propagacja w tył przez wszystkie warstwy\"\"\"\n",
    "        wyjscie_oczekiwane = wyjscie_oczekiwane.reshape(-1, 1)\n",
    "        \n",
    "        # Ostatnia warstwa\n",
    "        a_ost = self.stany[-1]['a']\n",
    "        net_ost = self.stany[-1]['net']\n",
    "        x_ost = self.stany[-1]['x']\n",
    "        \n",
    "        # δ[L] = (a[L] - y) ⊙ φ'(net[L])\n",
    "        dL_da = a_ost - wyjscie_oczekiwane\n",
    "        delta = dL_da * self.deriv_sigmoid(net_ost)\n",
    "        \n",
    "        # Aktualizacja wag ostatniej warstwy\n",
    "        dL_dW = delta @ x_ost.T\n",
    "        self.warstwy[-1] -= stala_uczenia * dL_dW # Aktualizujemy wagi od razu\n",
    "        \n",
    "        # Propagacja przez pozostałe warstwy\n",
    "        for l in range(len(self.warstwy)-2, -1, -1): # Iterujemy od przedostatniej warstwy do zerowej (cofamy się)\n",
    "            W_nastepna = self.warstwy[l+1] # Wagi bierzemy o warstwę wyższe\n",
    "            net_obecne = self.stany[l]['net']\n",
    "            x_obecne = self.stany[l]['x']\n",
    "            \n",
    "            # δ[l] = (W[l+1]ᵀ · δ[l+1]) ⊙ φ'(net[l]) - pomijamy bias\n",
    "\n",
    "            # Wykonujemy tu dwa działania jednocześnie dL/dX[l-1] = W[l].T @ dleta[l]\n",
    "            # Usuwając bias z dL/dX[l-1], otrzymamy dL/da[l-1]\n",
    "            # I liczymy delta[l-1] = dL/da[l-1] * fi'(net[l-1])\n",
    "\n",
    "            W_nastepna_bez_biasu = W_nastepna[:, 1:] # pomijamy bias\n",
    "            delta = (W_nastepna_bez_biasu.T @ delta) * self.deriv_sigmoid(net_obecne)\n",
    "            \n",
    "            # Aktualizacja wag\n",
    "            dL_dW = delta @ x_obecne.T\n",
    "            self.warstwy[l] -= stala_uczenia * dL_dW\n",
    "            \n",
    "\n",
    "    \n",
    "    def fit(self, X, y, epoki=10):\n",
    "        \"\"\"Uczenie sieci\"\"\"\n",
    "        for epoka in range(epoki):\n",
    "            total_loss = 0\n",
    "            for i in range(len(X)):\n",
    "                # Forward propagation\n",
    "                output = self.forward_prop(X[i])\n",
    "                \n",
    "                # Obliczanie straty\n",
    "                loss = 0.5 * np.sum((output - y[i]) ** 2)\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward propagation\n",
    "                self.back_prop(y[i])\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                avg_loss = total_loss / len(X)\n",
    "                print(f\"Epoka {epoka}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja\"\"\"\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            output = self.forward_prop(X[i])\n",
    "            predictions.append(output)\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Przykład użycia\n",
    "def test_sieci():\n",
    "    # Prosta sieć do testowania\n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(3, 2)  # 3 wejścia → 2 neurony\n",
    "    siec.dodaj_warstwe(2, 2)  # 2 wejścia → 2 neurony\n",
    "    \n",
    "    # Test forward propagation\n",
    "    wejscie = np.array([1, 1, 0])\n",
    "    print(\"Wejście:\", wejscie)\n",
    "    \n",
    "    output = siec.forward_prop(wejscie)\n",
    "    print(\"Wyjście:\", output)\n",
    "    \n",
    "    # Test backward propagation\n",
    "    y_oczekiwane = np.array([1, 0])\n",
    "    print(\"Oczekiwane:\", y_oczekiwane)\n",
    "    \n",
    "    siec.back_prop(y_oczekiwane)\n",
    "    print(\"Wagi po aktualizacji:\")\n",
    "    for i, w in enumerate(siec.warstwy):\n",
    "        print(f\"Warstwa {i}:\\n{w}\")\n",
    "\n",
    "# Test na danych MNIST\n",
    "def test_mnist():\n",
    "    # Przygotowanie danych (uproszczone)\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    print(\"Ładowanie danych MNIST...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    # Normalizacja i podział\n",
    "    X = X / 255.0\n",
    "    X_train, y_train = X[:1000], y[:1000]  # mały podzbiór\n",
    "    \n",
    "    # Konwersja etykiet na one-hot\n",
    "    y_train_onehot = np.zeros((len(y_train), 10))\n",
    "    y_train_onehot[np.arange(len(y_train)), y_train] = 1\n",
    "    \n",
    "    # Tworzenie sieci\n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(784, 128)\n",
    "    siec.dodaj_warstwe(128, 10)\n",
    "    \n",
    "    print(\"Rozpoczynam uczenie...\")\n",
    "    siec.fit(X_train, y_train_onehot, epoki=40)\n",
    "    \n",
    "    # Testowanie\n",
    "    correct = 0\n",
    "    for i in range(100):  # test na 100 przykładach\n",
    "        output = siec.forward_prop(X_train[i])\n",
    "        pred = np.argmax(output)\n",
    "        if pred == y_train[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / 100\n",
    "    print(f\"Dokładność: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Test prostej sieci:\")\n",
    "    test_sieci()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Test na danych MNIST:\")\n",
    "    test_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd013474",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data preprocessing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data = np.array(\u001b[43mdata\u001b[49m)\n\u001b[32m      3\u001b[39m m, n = data.shape\n\u001b[32m      4\u001b[39m np.random.shuffle(data)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4016019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_, m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182583b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "def init_params():\n",
    "    w1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    w2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80d247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_prop(w1, b1, w2, b2, X):\n",
    "    z1 = w1.dot(X) + b1\n",
    "    a1 = ReLU(z1)\n",
    "    z2 = w2.dot(a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6400273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66421f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "def back_prop(z1, a1, z2, a2, w2, Y, X):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    dZ2 = a2 - one_hot_Y\n",
    "    dW2 = 1/m * dZ2.dot(a1.T)\n",
    "    db2 = 1/m * np.sum(dZ2)\n",
    "    dZ1 = w2.T.dot(dZ2) * (z1 > 0)\n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e3cf257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter updates\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0f95b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def gradient_descent(X, Y, iterations, alpha):\n",
    "    w1, b1, w2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        z1, a1, z2, a2 = forward_prop(w1, b1, w2, b2, X)\n",
    "        dW1, db1, dW2, db2 = back_prop(z1, a1, z2, a2, w2, Y, X)\n",
    "        w1, b1, w2, b2 = update_params(w1, b1, w2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iterations: \", i)\n",
    "            print(\"Accuracy: \", get_accuracy(get_predictions(a2), Y))\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c8a9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = np.argmax(A2, 0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3da06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and evaluation\n",
    "def test_prediction(index, W1, b1, W2, b2):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459ecadd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_prediction(\u001b[32m0\u001b[39m, \u001b[43mw1\u001b[49m, b1, w2, b2)\n",
      "\u001b[31mNameError\u001b[39m: name 'w1' is not defined"
     ]
    }
   ],
   "source": [
    "test_prediction(0, w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaab533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e97e7d63",
   "metadata": {},
   "source": [
    "### Inne podejście"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3435f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        # Inicjalizacja wag i biasów\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Pamiętanie danych do propagacji wstecznej\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        \"\"\"Propagacja w przód\"\"\"\n",
    "        self.wejscie = wejscie  # zapamiętujemy wejście\n",
    "        # Obliczenia: wejście * wagi + bias\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Propagacja w tył\"\"\"\n",
    "        # Gradient względem wag\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        # Gradient względem biasów\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        # Gradient do poprzedniej warstwy\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        # Aktualizacja wag i biasów\n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Przepuszcza dane przez wszystkie warstwy\"\"\"\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Propaguje gradient przez wszystkie warstwy wstecz\"\"\"\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10):\n",
    "        \"\"\"Uczy sieć na danych\"\"\"\n",
    "        for epoka in range(epoki):\n",
    "            # Propagacja w przód\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Obliczenie błędu (różnica między przewidywaniem a prawdą)\n",
    "            error = output - y\n",
    "            \n",
    "            # Propagacja w tył\n",
    "            self.backward(error)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                loss = np.mean(error**2)\n",
    "                print(f\"Epoka {epoka}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Przewiduje dla nowych danych\"\"\"\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    # Dla softmax w połączeniu z cross-entropy, pochodna jest prosta\n",
    "    return 1  # Uproszczenie dla tego przykładu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57df1fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening...\n",
      "Epoka 0, Loss: 0.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dejvi\\AppData\\Local\\Temp\\ipykernel_23304\\90600536.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoka 10, Loss: 0.1806\n",
      "Epoka 20, Loss: 0.1806\n",
      "Epoka 30, Loss: 0.1820\n",
      "Epoka 40, Loss: 0.1820\n",
      "Dokładność na zbiorze testowym: 0.1135\n"
     ]
    }
   ],
   "source": [
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST\"\"\"\n",
    "    with gzip.open(sciezka, 'rb') as f:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    # Dane treningowe\n",
    "    X_train = train_set[0]\n",
    "    y_train = train_set[1]\n",
    "    \n",
    "    # Konwersja etykiet na one-hot encoding\n",
    "    y_train_onehot = np.zeros((y_train.size, 10))\n",
    "    y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return X_train, y_train_onehot\n",
    "\n",
    "# Główny program\n",
    "def main():\n",
    "    # Wczytaj dane\n",
    "    X_train, y_train = przygotuj_dane()\n",
    "    \n",
    "    # Stwórz sieć\n",
    "    siec = SiecNeuronowa()\n",
    "    \n",
    "    # Dodaj warstwy\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, learning_rate=0.01))  # Warstwa ukryta\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 10, learning_rate=0.01))   # Warstwa wyjściowa\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    # Trenuj sieć\n",
    "    print(\"Rozpoczynam trening...\")\n",
    "    siec.fit(X_train, y_train, epoki=50)\n",
    "    \n",
    "    # Testowanie\n",
    "    _, _, test_set = wczytaj_mnist()\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    \n",
    "    predictions = siec.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == y_test)\n",
    "    print(f\"Dokładność na zbiorze testowym: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ce34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksperyment 1: Różne rozmiary pierwszej warstwy\n",
    "def eksperyment_rozmiary():\n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\nTestowanie z warstwą ukrytą {rozmiar} neuronów:\")\n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        siec.fit(X_train, y_train, epoki=20)\n",
    "\n",
    "# Eksperyment 2: Dodanie trzeciej warstwy\n",
    "def trzy_warstwy():\n",
    "    print(\"\\nSieć z trzema warstwami ukrytymi:\")\n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))    # Dodatkowa warstwa\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    siec.fit(X_train, y_train, epoki=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a090966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testowanie z warstwą ukrytą 32 neuronów:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43meksperyment_rozmiary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36meksperyment_rozmiary\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m siec.dodaj_warstwe(Warstwa(rozmiar, \u001b[32m10\u001b[39m, \u001b[32m0.01\u001b[39m))\n\u001b[32m     10\u001b[39m siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m siec.fit(\u001b[43mX_train\u001b[49m, y_train, epoki=\u001b[32m20\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "eksperyment_rozmiary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f787fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - EKSPERYMENTY\n",
      "============================================================\n",
      "Wymiary danych treningowych: X: (50000, 784), y: (50000,)\n",
      "\n",
      "Wybierz eksperyment:\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "0 - Wyjście\n",
      "\n",
      "==================================================\n",
      "Sieć z trzema warstwami ukrytymi:\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50000,10) (50000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# 6. URUCHOMIENIE\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    186\u001b[39m     eksperyment_rozmiary()\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m wybor == \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mtrzy_warstwy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m wybor == \u001b[33m'\u001b[39m\u001b[33m3\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    190\u001b[39m     eksperyment_rozmiary()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mtrzy_warstwy\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    161\u001b[39m siec.dodaj_warstwe(Warstwa(\u001b[32m64\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m0.01\u001b[39m))\n\u001b[32m    162\u001b[39m siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43msiec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoki\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mSiecNeuronowa.fit\u001b[39m\u001b[34m(self, X, y, epoki)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoka \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoki):\n\u001b[32m     63\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.forward(X)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     error = \u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m.backward(error)\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoka % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (50000,10) (50000,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 1. KLASY SIECI NEURONOWEJ\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10):\n",
    "        for epoka in range(epoki):\n",
    "            output = self.forward(X)\n",
    "            error = output - y\n",
    "            self.backward(error)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                loss = np.mean(error**2)\n",
    "                print(f\"Epoka {epoka}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# 2. FUNKCJE AKTYWACJI\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    return 1\n",
    "\n",
    "# 3. FUNKCJE DO WCZYTANIA DANYCH\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST - jeśli nie masz pliku, użyj tymczasowych danych\"\"\"\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    \"\"\"Tworzy proste dane do testowania jeśli nie ma pliku MNIST\"\"\"\n",
    "    # Proste dane treningowe (100 próbek)\n",
    "    X_train = np.random.rand(100, 784)\n",
    "    y_train = np.random.randint(0, 10, 100)\n",
    "    y_train_onehot = np.zeros((100, 10))\n",
    "    y_train_onehot[np.arange(100), y_train] = 1\n",
    "    \n",
    "    # Proste dane testowe (20 próbek)\n",
    "    X_test = np.random.rand(20, 784)\n",
    "    y_test = np.random.randint(0, 10, 20)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:  # Jeśli to nasze tymczasowe dane\n",
    "        X_train = train_set[0]\n",
    "        y_train_onehot = train_set[1]\n",
    "    else:  # Jeśli to prawdziwe dane MNIST\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        y_train_onehot = np.zeros((y_train.size, 10))\n",
    "        y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return X_train, y_train_onehot\n",
    "\n",
    "# 4. EKSPERYMENTY\n",
    "def eksperyment_rozmiary():\n",
    "    \"\"\"Eksperyment 1: Różne rozmiary pierwszej warstwy\"\"\"\n",
    "    X_train, y_train = przygotuj_dane()\n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testowanie z warstwą ukrytą {rozmiar} neuronów:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        siec.fit(X_train, y_train, epoki=20)\n",
    "\n",
    "def trzy_warstwy():\n",
    "    \"\"\"Eksperyment 2: Dodanie trzeciej warstwy\"\"\"\n",
    "    X_train, y_train = przygotuj_dane()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Sieć z trzema warstwami ukrytymi:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))    # Dodatkowa warstwa\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    siec.fit(X_train, y_train, epoki=30)\n",
    "\n",
    "# 5. GŁÓWNY PROGRAM\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - EKSPERYMENTY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sprawdź czy dane ładują się poprawnie\n",
    "    X_train, y_train = przygotuj_dane()\n",
    "    print(f\"Wymiary danych treningowych: X: {X_train.shape}, y: {y_train.shape}\")\n",
    "    \n",
    "    # Wybierz który eksperyment uruchomić\n",
    "    while True:\n",
    "        print(\"\\nWybierz eksperyment:\")\n",
    "        print(\"1 - Różne rozmiary pierwszej warstwy\")\n",
    "        print(\"2 - Trzy warstwy ukryte\")\n",
    "        print(\"3 - Oba eksperymenty\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        \n",
    "        wybor = input(\"Twój wybór (0-3): \").strip()\n",
    "        \n",
    "        if wybor == '1':\n",
    "            eksperyment_rozmiary()\n",
    "        elif wybor == '2':\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '3':\n",
    "            eksperyment_rozmiary()\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '0':\n",
    "            print(\"Koniec programu.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Nieprawidłowy wybór. Spróbuj ponownie.\")\n",
    "\n",
    "# 6. URUCHOMIENIE\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeef3f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - EKSPERYMENTY\n",
      "============================================================\n",
      "Wymiary danych treningowych: X: (50000, 784), y: (50000, 10)\n",
      "\n",
      "Wybierz eksperyment:\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "0 - Wyjście\n",
      "Wymiary podzbioru: X: (1000, 784), y: (1000, 10)\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 32 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0917\n",
      "Epoka 10, Loss: 0.0790\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 64 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0924\n",
      "Epoka 10, Loss: 0.0966\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 128 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0938\n",
      "Epoka 10, Loss: 0.0743\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 256 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.1021\n",
      "Epoka 10, Loss: 0.0854\n",
      "\n",
      "Wybierz eksperyment:\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "0 - Wyjście\n",
      "Wymiary podzbioru: X: (1000, 784), y: (1000, 10)\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 32 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0903\n",
      "Epoka 10, Loss: 0.0800\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 64 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0913\n",
      "Epoka 10, Loss: 0.0809\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 128 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0931\n",
      "Epoka 10, Loss: 0.0839\n",
      "\n",
      "==================================================\n",
      "Testowanie z warstwą ukrytą 256 neuronów:\n",
      "==================================================\n",
      "Epoka 0, Loss: 0.0918\n",
      "Epoka 10, Loss: 0.0776\n",
      "\n",
      "Wybierz eksperyment:\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "0 - Wyjście\n",
      "Koniec programu.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 1. KLASY SIECI NEURONOWEJ\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10):\n",
    "        for epoka in range(epoki):\n",
    "            output = self.forward(X)\n",
    "            error = output - y\n",
    "            self.backward(error)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                loss = np.mean(error**2)\n",
    "                print(f\"Epoka {epoka}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# 2. FUNKCJE AKTYWACJI\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    return 1\n",
    "\n",
    "# 3. FUNKCJE DO WCZYTANIA DANYCH\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST\"\"\"\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    \"\"\"Tworzy proste dane do testowania jeśli nie ma pliku MNIST\"\"\"\n",
    "    # Proste dane treningowe (1000 próbek dla szybszego testowania)\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    # Proste dane testowe (200 próbek)\n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia - ZWRACA JEDNOLICIE one-hot encoding\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:  # Jeśli to nasze tymczasowe dane\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        # Konwersja na one-hot\n",
    "        y_train_onehot = np.zeros((y_train.size, 10))\n",
    "        y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    else:  # Jeśli to prawdziwe dane MNIST\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        # Konwersja na one-hot\n",
    "        y_train_onehot = np.zeros((y_train.size, 10))\n",
    "        y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return X_train, y_train_onehot\n",
    "\n",
    "# 4. EKSPERYMENTY - POPRAWIONE WERSJE\n",
    "def eksperyment_rozmiary():\n",
    "    \"\"\"Eksperyment 1: Różne rozmiary pierwszej warstwy\"\"\"\n",
    "    X_train, y_train_onehot = przygotuj_dane()  # Teraz zawsze one-hot\n",
    "    \n",
    "    # Użyj mniejszego podzbioru danych dla szybszego testowania\n",
    "    X_train_subset = X_train[:1000]  # Pierwsze 1000 próbek\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    print(f\"Wymiary podzbioru: X: {X_train_subset.shape}, y: {y_train_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testowanie z warstwą ukrytą {rozmiar} neuronów:\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        siec.fit(X_train_subset, y_train_subset, epoki=20)\n",
    "\n",
    "def trzy_warstwy():\n",
    "    \"\"\"Eksperyment 2: Dodanie trzeciej warstwy\"\"\"\n",
    "    X_train, y_train_onehot = przygotuj_dane()  # Teraz zawsze one-hot\n",
    "    \n",
    "    # Użyj mniejszego podzbioru danych dla szybszego testowania\n",
    "    X_train_subset = X_train[:1000]  # Pierwsze 1000 próbek\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    print(f\"Wymiary podzbioru: X: {X_train_subset.shape}, y: {y_train_subset.shape}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Sieć z trzema warstwami ukrytymi:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))    # Dodatkowa warstwa\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    siec.fit(X_train_subset, y_train_subset, epoki=30)\n",
    "\n",
    "# 5. DODATKOWA FUNKCJA DO TESTOWANIA DOKŁADNOŚCI\n",
    "def testuj_dokladnosc(siec, X_test, y_test):\n",
    "    \"\"\"Testuje dokładność sieci na danych testowych\"\"\"\n",
    "    predictions = siec.predict(X_test)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == y_test)\n",
    "    return accuracy\n",
    "\n",
    "# 6. GŁÓWNY PROGRAM\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - EKSPERYMENTY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sprawdź czy dane ładują się poprawnie\n",
    "    X_train, y_train_onehot = przygotuj_dane()\n",
    "    print(f\"Wymiary danych treningowych: X: {X_train.shape}, y: {y_train_onehot.shape}\")\n",
    "    \n",
    "    # Wybierz który eksperyment uruchomić\n",
    "    while True:\n",
    "        print(\"\\nWybierz eksperyment:\")\n",
    "        print(\"1 - Różne rozmiary pierwszej warstwy\")\n",
    "        print(\"2 - Trzy warstwy ukryte\")\n",
    "        print(\"3 - Oba eksperymenty\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        \n",
    "        wybor = input(\"Twój wybór (0-3): \").strip()\n",
    "        \n",
    "        if wybor == '1':\n",
    "            eksperyment_rozmiary()\n",
    "        elif wybor == '2':\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '3':\n",
    "            eksperyment_rozmiary()\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '0':\n",
    "            print(\"Koniec programu.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Nieprawidłowy wybór. Spróbuj ponownie.\")\n",
    "\n",
    "# 7. URUCHOMIENIE\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd981eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPROSZCZONY TEST\n",
    "def prosty_test():\n",
    "    \"\"\"Uproszczony test na małych danych\"\"\"\n",
    "    # Stwórz bardzo małe dane testowe\n",
    "    X_test = np.random.rand(100, 784)  # 100 próbek\n",
    "    y_test = np.random.randint(0, 10, 100)\n",
    "    y_test_onehot = np.zeros((100, 10))\n",
    "    y_test_onehot[np.arange(100), y_test] = 1\n",
    "    \n",
    "    print(\"Uproszczony test:\")\n",
    "    print(f\"X shape: {X_test.shape}, y shape: {y_test_onehot.shape}\")\n",
    "    \n",
    "    # Prosta sieć\n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 64, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    print(\"Rozpoczynam trening...\")\n",
    "    siec.fit(X_test, y_test_onehot, epoki=10)\n",
    "    print(\"Test zakończony sukcesem!\")\n",
    "\n",
    "# W main() dodaj opcję 4 dla prostego testu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81aade39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - EKSPERYMENTY\n",
      "============================================================\n",
      "Pełne dane: X: (50000, 784), y: (50000, 10)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE - Wybierz eksperyment:\n",
      "==================================================\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "4 - Szybki test (100 próbek)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "============================================================\n",
      "SZYBKI TEST (100 próbek, 10 epok)\n",
      "============================================================\n",
      "Epoka 0, Loss: 0.0895\n",
      "Szybki test zakończony!\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE - Wybierz eksperyment:\n",
      "==================================================\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "4 - Szybki test (100 próbek)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Wymiary podzbioru: X: (1000, 784), y: (1000, 10)\n",
      "\n",
      "============================================================\n",
      "TEST: Warstwa ukryta 32 neuronów\n",
      "============================================================\n",
      "Epoka 0, Loss: 0.0903\n",
      "Epoka 10, Loss: 0.0853\n",
      "Epoka 20, Loss: 0.0797\n",
      "Final Loss dla 32 neuronów: 0.0751\n",
      "\n",
      "============================================================\n",
      "TEST: Warstwa ukryta 64 neuronów\n",
      "============================================================\n",
      "Epoka 0, Loss: 0.0922\n",
      "Epoka 10, Loss: 0.0847\n",
      "Epoka 20, Loss: 0.0950\n",
      "Final Loss dla 64 neuronów: 0.0579\n",
      "\n",
      "============================================================\n",
      "TEST: Warstwa ukryta 128 neuronów\n",
      "============================================================\n",
      "Epoka 0, Loss: 0.0926\n",
      "Epoka 10, Loss: 0.0884\n",
      "Epoka 20, Loss: 0.0869\n",
      "Final Loss dla 128 neuronów: 0.0779\n",
      "\n",
      "============================================================\n",
      "TEST: Warstwa ukryta 256 neuronów\n",
      "============================================================\n",
      "Epoka 0, Loss: 0.0965\n",
      "Epoka 10, Loss: 0.0786\n",
      "Epoka 20, Loss: 0.0698\n",
      "Final Loss dla 256 neuronów: 0.0657\n",
      "\n",
      "============================================================\n",
      "PODSUMOWANIE EKSPERYMENTU 1:\n",
      "============================================================\n",
      "Rozmiar 32: Final Loss = 0.0751\n",
      "Rozmiar 64: Final Loss = 0.0579\n",
      "Rozmiar 128: Final Loss = 0.0779\n",
      "Rozmiar 256: Final Loss = 0.0657\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE - Wybierz eksperyment:\n",
      "==================================================\n",
      "1 - Różne rozmiary pierwszej warstwy\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Oba eksperymenty\n",
      "4 - Szybki test (100 próbek)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "Dziękuję za użycie programu! Do widzenia!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 1. KLASY SIECI NEURONOWEJ\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10):\n",
    "        losses = []\n",
    "        for epoka in range(epoki):\n",
    "            output = self.forward(X)\n",
    "            error = output - y\n",
    "            self.backward(error)\n",
    "            \n",
    "            loss = np.mean(error**2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                print(f\"Epoka {epoka}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# 2. FUNKCJE AKTYWACJI\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    return 1\n",
    "\n",
    "# 3. FUNKCJE DO WCZYTANIA DANYCH\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST\"\"\"\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    \"\"\"Tworzy proste dane do testowania jeśli nie ma pliku MNIST\"\"\"\n",
    "    np.random.seed(42)  # Dla powtarzalności\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        y_train_onehot = np.zeros((y_train.size, 10))\n",
    "        y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    else:\n",
    "        X_train = train_set[0]\n",
    "        y_train = train_set[1]\n",
    "        y_train_onehot = np.zeros((y_train.size, 10))\n",
    "        y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return X_train, y_train_onehot\n",
    "\n",
    "# 4. EKSPERYMENTY - ULEPSZONE\n",
    "def eksperyment_rozmiary():\n",
    "    \"\"\"Eksperyment 1: Różne rozmiary pierwszej warstwy\"\"\"\n",
    "    X_train, y_train_onehot = przygotuj_dane()\n",
    "    \n",
    "    # Użyj mniejszego podzbioru dla szybkości\n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    print(f\"Wymiary podzbioru: X: {X_train_subset.shape}, y: {y_train_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    wyniki = {}\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST: Warstwa ukryta {rozmiar} neuronów\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        losses = siec.fit(X_train_subset, y_train_subset, epoki=30)\n",
    "        wyniki[rozmiar] = losses[-1]  # Zapisz finalny loss\n",
    "        \n",
    "        print(f\"Final Loss dla {rozmiar} neuronów: {losses[-1]:.4f}\")\n",
    "    \n",
    "    # Podsumowanie\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PODSUMOWANIE EKSPERYMENTU 1:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for rozmiar, loss in wyniki.items():\n",
    "        print(f\"Rozmiar {rozmiar}: Final Loss = {loss:.4f}\")\n",
    "    \n",
    "    return wyniki\n",
    "\n",
    "def trzy_warstwy():\n",
    "    \"\"\"Eksperyment 2: Dodanie trzeciej warstwy\"\"\"\n",
    "    X_train, y_train_onehot = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    print(f\"Wymiary podzbioru: X: {X_train_subset.shape}, y: {y_train_subset.shape}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EKSPERYMENT 2: Trzy warstwy ukryte\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    losses = siec.fit(X_train_subset, y_train_subset, epoki=30)\n",
    "    print(f\"Final Loss dla 3 warstw: {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses[-1]\n",
    "\n",
    "def szybki_test():\n",
    "    \"\"\"Szybki test na bardzo małych danych\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SZYBKI TEST (100 próbek, 10 epok)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Bardzo małe dane\n",
    "    X_test = np.random.rand(100, 784)\n",
    "    y_test = np.random.randint(0, 10, 100)\n",
    "    y_test_onehot = np.zeros((100, 10))\n",
    "    y_test_onehot[np.arange(100), y_test] = 1\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 32, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(32, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    siec.fit(X_test, y_test_onehot, epoki=10)\n",
    "    print(\"Szybki test zakończony!\")\n",
    "\n",
    "# 5. GŁÓWNY PROGRAM\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - EKSPERYMENTY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    X_train, y_train_onehot = przygotuj_dane()\n",
    "    print(f\"Pełne dane: X: {X_train.shape}, y: {y_train_onehot.shape}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MENU GŁÓWNE - Wybierz eksperyment:\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1 - Różne rozmiary pierwszej warstwy\")\n",
    "        print(\"2 - Trzy warstwy ukryte\") \n",
    "        print(\"3 - Oba eksperymenty\")\n",
    "        print(\"4 - Szybki test (100 próbek)\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            wybor = input(\"Twój wybór (0-4): \").strip()\n",
    "            \n",
    "            if wybor == '1':\n",
    "                eksperyment_rozmiary()\n",
    "            elif wybor == '2':\n",
    "                trzy_warstwy()\n",
    "            elif wybor == '3':\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"URUCHAMIANIE OBIU EKSPERYMENTÓW\")\n",
    "                print(\"=\"*60)\n",
    "                wyniki1 = eksperyment_rozmiary()\n",
    "                wyniki2 = trzy_warstwy()\n",
    "            elif wybor == '4':\n",
    "                szybki_test()\n",
    "            elif wybor == '0':\n",
    "                print(\"\\nDziękuję za użycie programu! Do widzenia!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Nieprawidłowy wybór. Wybierz 0-4.\")\n",
    "                \n",
    "            # Potwierdzenie kontynuacji\n",
    "            if wybor != '0':\n",
    "                input(\"\\nNaciśnij Enter aby kontynuować...\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nPrzerwano przez użytkownika. Do widzenia!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWystąpił błąd: {e}\")\n",
    "            print(\"Spróbuj ponownie.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7feb4b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - EKSPERYMENTY Z DOKŁADNOŚCIĄ\n",
      "======================================================================\n",
      "Dane: Train: (50000, 784), Val: (10000, 784), Test: (10000, 784)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAxisError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 327\u001b[39m\n\u001b[32m    324\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 289\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    286\u001b[39m wybor = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTwój wybór (0-4): \u001b[39m\u001b[33m\"\u001b[39m).strip()\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wybor == \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[43meksperyment_rozmiary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m wybor == \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    291\u001b[39m     trzy_warstwy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36meksperyment_rozmiary\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    186\u001b[39m siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Trenuj z walidacją\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m losses, accuracies = \u001b[43msiec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mX_val_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoki\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Testuj na danych testowych\u001b[39;00m\n\u001b[32m    193\u001b[39m test_accuracy = siec.accuracy(X_test[:\u001b[32m200\u001b[39m], y_test[:\u001b[32m200\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mSiecNeuronowa.fit\u001b[39m\u001b[34m(self, X, y, X_val, y_val, epoki)\u001b[39m\n\u001b[32m     84\u001b[39m val_accuracy = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     val_acc = \u001b[38;5;28mself\u001b[39m.accuracy(X_val, \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     87\u001b[39m     val_accuracy = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoka \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoka\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:1341\u001b[39m, in \u001b[36margmax\u001b[39m\u001b[34m(a, axis, out, keepdims)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1253\u001b[39m \u001b[33;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[32m   1254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1338\u001b[39m \u001b[33;03m(2, 1, 4)\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m kwds = {\u001b[33m'\u001b[39m\u001b[33mkeepdims\u001b[39m\u001b[33m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np._NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mAxisError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 1. KLASY SIECI NEURONOWEJ\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoki=10):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Propagacja w przód\n",
    "            output = self.forward(X)\n",
    "            error = output - y\n",
    "            \n",
    "            # Propagacja w tył\n",
    "            self.backward(error)\n",
    "            \n",
    "            # Obliczanie metryk\n",
    "            loss = np.mean(error**2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Obliczanie dokładności\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            true_labels = np.argmax(y, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                val_accuracy = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc = self.accuracy(X_val, np.argmax(y_val, axis=1))\n",
    "                    val_accuracy = f\", Val Accuracy: {val_acc:.4f}\"\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {loss:.4f}, Train Accuracy: {accuracy:.4f}{val_accuracy}\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Oblicza dokładność na danych testowych\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        return np.mean(predicted_labels == y_true)\n",
    "\n",
    "# 2. FUNKCJE AKTYWACJI\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    return 1\n",
    "\n",
    "# 3. FUNKCJE DO WCZYTANIA DANYCH\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST\"\"\"\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    \"\"\"Tworzy proste dane do testowania\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia z podziałem na train/val/test\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:  # Tymczasowe dane\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set\n",
    "        X_test, y_test = test_set\n",
    "    else:  # Prawdziwe MNIST\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set  \n",
    "        X_test, y_test = test_set\n",
    "    \n",
    "    # Konwersja na one-hot tylko dla treningu\n",
    "    y_train_onehot = np.zeros((y_train.size, 10))\n",
    "    y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# 4. EKSPERYMENTY Z DOKŁADNOŚCIĄ\n",
    "def eksperyment_rozmiary():\n",
    "    \"\"\"Eksperyment 1: Różne rozmiary pierwszej warstwy\"\"\"\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    # Użyj mniejszego podzbioru dla szybkości\n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    y_train_labels_subset = y_train[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]\n",
    "    \n",
    "    print(f\"Train: {X_train_subset.shape}, Val: {X_val_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    wyniki = {}\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TEST: Warstwa ukryta {rozmiar} neuronów\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        # Trenuj z walidacją\n",
    "        losses, accuracies = siec.fit(X_train_subset, y_train_subset, \n",
    "                                    X_val_subset, y_val_subset, epoki=30)\n",
    "        \n",
    "        # Testuj na danych testowych\n",
    "        test_accuracy = siec.accuracy(X_test[:200], y_test[:200])\n",
    "        \n",
    "        wyniki[rozmiar] = {\n",
    "            'final_loss': losses[-1],\n",
    "            'final_train_acc': accuracies[-1],\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # PODSUMOWANIE\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PODSUMOWANIE EKSPERYMENTU 1 - RÓŻNE ROZMIARY WARSTWY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Rozmiar | Train Acc | Test Acc  | Final Loss\")\n",
    "    print(\"-\" * 50)\n",
    "    for rozmiar, metryki in wyniki.items():\n",
    "        print(f\"{rozmiar:7d} | {metryki['final_train_acc']:9.4f} | {metryki['test_accuracy']:9.4f} | {metryki['final_loss']:10.4f}\")\n",
    "    \n",
    "    # Znajdź najlepszy rozmiar\n",
    "    najlepszy = max(wyniki.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    print(f\"\\n🏆 NAJLEPSZY: {najlepszy[0]} neuronów (Test Acc: {najlepszy[1]['test_accuracy']:.4f})\")\n",
    "    \n",
    "    return wyniki\n",
    "\n",
    "def trzy_warstwy():\n",
    "    \"\"\"Eksperyment 2: Dodanie trzeciej warstwy\"\"\"\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    y_train_labels_subset = y_train[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EKSPERYMENT 2: TRZY WARSTWY UKRYTE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Architektura: 784 → 128 → 64 → 10\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    losses, accuracies = siec.fit(X_train_subset, y_train_subset, \n",
    "                                X_val_subset, y_val_subset, epoki=30)\n",
    "    \n",
    "    test_accuracy = siec.accuracy(X_test[:200], y_test[:200])\n",
    "    \n",
    "    print(f\"\\n⟹ WYNIK: Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "def pokaz_przyklady(siec, X_test, y_test, ile=5):\n",
    "    \"\"\"Pokazuje przykładowe predykcje\"\"\"\n",
    "    print(f\"\\nPrzykładowe predykcje (pierwszych {ile}):\")\n",
    "    print(\"True | Pred | ✓/✗\")\n",
    "    print(\"-\" * 15)\n",
    "    \n",
    "    predictions = siec.predict(X_test[:ile])\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    for i in range(ile):\n",
    "        true_label = y_test[i]\n",
    "        pred_label = predicted_labels[i]\n",
    "        correct = \"✓\" if true_label == pred_label else \"✗\"\n",
    "        print(f\"{true_label:4} | {pred_label:4} | {correct}\")\n",
    "\n",
    "# 5. GŁÓWNY PROGRAM\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - EKSPERYMENTY Z DOKŁADNOŚCIĄ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    print(f\"Dane: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MENU GŁÓWNE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1 - Różne rozmiary warstwy (z dokładnością)\")\n",
    "        print(\"2 - Trzy warstwy ukryte (z dokładnością)\") \n",
    "        print(\"3 - Oba eksperymenty\")\n",
    "        print(\"4 - Pokaż przykładowe predykcje\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            wybor = input(\"Twój wybór (0-4): \").strip()\n",
    "            \n",
    "            if wybor == '1':\n",
    "                eksperyment_rozmiary()\n",
    "            elif wybor == '2':\n",
    "                trzy_warstwy()\n",
    "            elif wybor == '3':\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"URUCHAMIANIE OBIU EKSPERYMENTÓW\")\n",
    "                print(\"=\"*70)\n",
    "                wyniki1 = eksperyment_rozmiary()\n",
    "                wyniki2 = trzy_warstwy()\n",
    "            elif wybor == '4':\n",
    "                # Prosta sieć do pokazania przykładów\n",
    "                siec = SiecNeuronowa()\n",
    "                siec.dodaj_warstwe(Warstwa(784, 64, 0.01))\n",
    "                siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "                siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "                siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "                \n",
    "                # Wytrenuj szybko\n",
    "                X_temp = X_train[:100]\n",
    "                y_temp = np.zeros((100, 10))\n",
    "                y_temp[np.arange(100), y_train[:100]] = 1\n",
    "                siec.fit(X_temp, y_temp, epoki=5)\n",
    "                \n",
    "                pokaz_przyklady(siec, X_test, y_test, 10)\n",
    "            elif wybor == '0':\n",
    "                print(\"\\nDo widzenia!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Nieprawidłowy wybór.\")\n",
    "                \n",
    "            if wybor != '0':\n",
    "                input(\"\\nNaciśnij Enter aby kontynuować...\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nPrzerwano. Do widzenia!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54bc8c",
   "metadata": {},
   "source": [
    "# TA WERSJA DZIAŁA OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f96ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - EKSPERYMENTY Z DOKŁADNOŚCIĄ\n",
      "======================================================================\n",
      "Dane: Train: (50000, 784), Val: (10000, 784), Test: (10000, 784)\n",
      "Etykiety - Train: (50000,), Val: (10000,), Test: (10000,)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0904, Train Accuracy: 0.1080, Val Accuracy: 0.1000\n",
      "Epoka  10, Loss: 0.0790, Train Accuracy: 0.3140, Val Accuracy: 0.4150\n",
      "Epoka  20, Loss: 0.0695, Train Accuracy: 0.4150, Val Accuracy: 0.3700\n",
      "⟹ Final - Train Acc: 0.4890, Test Acc: 0.3750\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 64 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0911, Train Accuracy: 0.1270, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0825, Train Accuracy: 0.2410, Val Accuracy: 0.1650\n",
      "Epoka  20, Loss: 0.0892, Train Accuracy: 0.1300, Val Accuracy: 0.2300\n",
      "⟹ Final - Train Acc: 0.3500, Test Acc: 0.3550\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 128 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0912, Train Accuracy: 0.1060, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0902, Train Accuracy: 0.1220, Val Accuracy: 0.1650\n",
      "Epoka  20, Loss: 0.0918, Train Accuracy: 0.1160, Val Accuracy: 0.1450\n",
      "⟹ Final - Train Acc: 0.2230, Test Acc: 0.2650\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 256 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0931, Train Accuracy: 0.1140, Val Accuracy: 0.1100\n",
      "Epoka  10, Loss: 0.0788, Train Accuracy: 0.3410, Val Accuracy: 0.2350\n",
      "Epoka  20, Loss: 0.0734, Train Accuracy: 0.4040, Val Accuracy: 0.2250\n",
      "⟹ Final - Train Acc: 0.4470, Test Acc: 0.4500\n",
      "\n",
      "======================================================================\n",
      "PODSUMOWANIE EKSPERYMENTU 1 - RÓŻNE ROZMIARY WARSTWY\n",
      "======================================================================\n",
      "Rozmiar | Train Acc | Test Acc  | Final Loss\n",
      "--------------------------------------------------\n",
      "     32 |    0.4890 |    0.3750 |     0.0657\n",
      "     64 |    0.3500 |    0.3550 |     0.0713\n",
      "    128 |    0.2230 |    0.2650 |     0.0828\n",
      "    256 |    0.4470 |    0.4500 |     0.0659\n",
      "\n",
      "🏆 NAJLEPSZY: 256 neuronów (Test Acc: 0.4500)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "EKSPERYMENT 2: TRZY WARSTWY UKRYTE\n",
      "======================================================================\n",
      "Architektura: 784 → 128 → 64 → 10\n",
      "Epoka   0, Loss: 0.0902, Train Accuracy: 0.1050, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0899, Train Accuracy: 0.2020, Val Accuracy: 0.1300\n",
      "Epoka  20, Loss: 0.0899, Train Accuracy: 0.1960, Val Accuracy: 0.1650\n",
      "\n",
      "⟹ WYNIK: Train Acc: 0.2220, Test Acc: 0.2550\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "URUCHAMIANIE OBIU EKSPERYMENTÓW\n",
      "======================================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0905, Train Accuracy: 0.0940, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0893, Train Accuracy: 0.1400, Val Accuracy: 0.3150\n",
      "Epoka  20, Loss: 0.0870, Train Accuracy: 0.2300, Val Accuracy: 0.2950\n",
      "⟹ Final - Train Acc: 0.5600, Test Acc: 0.4750\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 64 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0916, Train Accuracy: 0.0920, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0797, Train Accuracy: 0.2160, Val Accuracy: 0.2500\n",
      "Epoka  20, Loss: 0.0754, Train Accuracy: 0.3070, Val Accuracy: 0.3100\n",
      "⟹ Final - Train Acc: 0.4560, Test Acc: 0.3050\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 128 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0929, Train Accuracy: 0.1080, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0813, Train Accuracy: 0.2220, Val Accuracy: 0.1700\n",
      "Epoka  20, Loss: 0.0833, Train Accuracy: 0.2160, Val Accuracy: 0.1850\n",
      "⟹ Final - Train Acc: 0.2950, Test Acc: 0.4500\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 256 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 0.0937, Train Accuracy: 0.1710, Val Accuracy: 0.1150\n",
      "Epoka  10, Loss: 0.0828, Train Accuracy: 0.2820, Val Accuracy: 0.2100\n",
      "Epoka  20, Loss: 0.0718, Train Accuracy: 0.3610, Val Accuracy: 0.3350\n",
      "⟹ Final - Train Acc: 0.4210, Test Acc: 0.3750\n",
      "\n",
      "======================================================================\n",
      "PODSUMOWANIE EKSPERYMENTU 1 - RÓŻNE ROZMIARY WARSTWY\n",
      "======================================================================\n",
      "Rozmiar | Train Acc | Test Acc  | Final Loss\n",
      "--------------------------------------------------\n",
      "     32 |    0.5600 |    0.4750 |     0.0599\n",
      "     64 |    0.4560 |    0.3050 |     0.0709\n",
      "    128 |    0.2950 |    0.4500 |     0.0782\n",
      "    256 |    0.4210 |    0.3750 |     0.0653\n",
      "\n",
      "🏆 NAJLEPSZY: 32 neuronów (Test Acc: 0.4750)\n",
      "\n",
      "======================================================================\n",
      "EKSPERYMENT 2: TRZY WARSTWY UKRYTE\n",
      "======================================================================\n",
      "Architektura: 784 → 128 → 64 → 10\n",
      "Epoka   0, Loss: 0.0901, Train Accuracy: 0.1200, Val Accuracy: 0.0900\n",
      "Epoka  10, Loss: 0.0899, Train Accuracy: 0.1180, Val Accuracy: 0.1450\n",
      "Epoka  20, Loss: 0.0899, Train Accuracy: 0.1610, Val Accuracy: 0.1450\n",
      "\n",
      "⟹ WYNIK: Train Acc: 0.2210, Test Acc: 0.2450\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Epoka   0, Loss: 0.0932, Train Accuracy: 0.0100\n",
      "\n",
      "Przykładowe predykcje (pierwszych 10):\n",
      "True | Pred | ✓/✗\n",
      "---------------\n",
      "   7 |    1 | ✗\n",
      "   2 |    1 | ✗\n",
      "   1 |    1 | ✓\n",
      "   0 |    0 | ✓\n",
      "   4 |    4 | ✓\n",
      "   1 |    1 | ✓\n",
      "   4 |    1 | ✗\n",
      "   9 |    1 | ✗\n",
      "   5 |    1 | ✗\n",
      "   9 |    9 | ✓\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy (z dokładnością)\n",
      "2 - Trzy warstwy ukryte (z dokładnością)\n",
      "3 - Oba eksperymenty\n",
      "4 - Pokaż przykładowe predykcje\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "Do widzenia!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# 1. KLASY SIECI NEURONOWEJ\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.wagi = np.random.randn(rozmiar_wejscia, rozmiar_wyjscia) * 0.1\n",
    "        self.bias = np.zeros((1, rozmiar_wyjscia))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = np.dot(wejscie, self.wagi) + self.bias\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_wag = np.dot(self.wejscie.T, gradient)\n",
    "        gradient_bias = np.sum(gradient, axis=0, keepdims=True)\n",
    "        gradient_wejscie = np.dot(gradient, self.wagi.T)\n",
    "        \n",
    "        self.wagi -= self.learning_rate * gradient_wag\n",
    "        self.bias -= self.learning_rate * gradient_bias\n",
    "        \n",
    "        return gradient_wejscie\n",
    "\n",
    "class WarstwaAktywacji:\n",
    "    def __init__(self, funkcja_aktywacji, pochodna_funkcji):\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.pochodna_funkcji = pochodna_funkcji\n",
    "        self.wejscie = None\n",
    "        self.wyjscie = None\n",
    "    \n",
    "    def forward(self, wejscie):\n",
    "        self.wejscie = wejscie\n",
    "        self.wyjscie = self.funkcja_aktywacji(wejscie)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        return gradient * self.pochodna_funkcji(self.wejscie)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for warstwa in self.warstwy:\n",
    "            output = warstwa.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        for warstwa in reversed(self.warstwy):\n",
    "            gradient = warstwa.backward(gradient)\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epoki=10):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Propagacja w przód\n",
    "            output = self.forward(X)\n",
    "            error = output - y\n",
    "            \n",
    "            # Propagacja w tył\n",
    "            self.backward(error)\n",
    "            \n",
    "            # Obliczanie metryk\n",
    "            loss = np.mean(error**2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Obliczanie dokładności\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            true_labels = np.argmax(y, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            if epoka % 10 == 0:\n",
    "                val_accuracy = \"\"\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    val_acc = self.accuracy(X_val, y_val)  # POPRAWIONE: używamy y_val bez argmax\n",
    "                    val_accuracy = f\", Val Accuracy: {val_acc:.4f}\"\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {loss:.4f}, Train Accuracy: {accuracy:.4f}{val_accuracy}\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Oblicza dokładność na danych testowych - y_true to już etykiety, nie one-hot\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        return np.mean(predicted_labels == y_true)\n",
    "\n",
    "# 2. FUNKCJE AKTYWACJI\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def pochodna_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def pochodna_softmax(x):\n",
    "    return 1\n",
    "\n",
    "# 3. FUNKCJE DO WCZYTANIA DANYCH\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    \"\"\"Wczytuje dane MNIST\"\"\"\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    \"\"\"Tworzy proste dane do testowania\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    \"\"\"Przygotowuje dane do uczenia z podziałem na train/val/test\"\"\"\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:  # Tymczasowe dane\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set\n",
    "        X_test, y_test = test_set\n",
    "    else:  # Prawdziwe MNIST\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set  \n",
    "        X_test, y_test = test_set\n",
    "    \n",
    "    # Konwersja na one-hot tylko dla treningu\n",
    "    y_train_onehot = np.zeros((y_train.size, 10))\n",
    "    y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# 4. EKSPERYMENTY Z DOKŁADNOŚCIĄ - POPRAWIONE\n",
    "def eksperyment_rozmiary():\n",
    "    \"\"\"Eksperyment 1: Różne rozmiary pierwszej warstwy\"\"\"\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    # Użyj mniejszego podzbioru dla szybkości\n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    y_train_labels_subset = y_train[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]  # To są już etykiety, nie one-hot\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"Train: {X_train_subset.shape}, Val: {X_val_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    wyniki = {}\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TEST: Warstwa ukryta {rozmiar} neuronów\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.01))\n",
    "        siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "        \n",
    "        # Trenuj z walidacją - POPRAWIONE: przekazujemy y_val_subset jako etykiety\n",
    "        losses, accuracies = siec.fit(X_train_subset, y_train_subset, \n",
    "                                    X_val_subset, y_val_subset, epoki=30)\n",
    "        \n",
    "        # Testuj na danych testowych\n",
    "        test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "        \n",
    "        wyniki[rozmiar] = {\n",
    "            'final_loss': losses[-1],\n",
    "            'final_train_acc': accuracies[-1],\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # PODSUMOWANIE\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PODSUMOWANIE EKSPERYMENTU 1 - RÓŻNE ROZMIARY WARSTWY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Rozmiar | Train Acc | Test Acc  | Final Loss\")\n",
    "    print(\"-\" * 50)\n",
    "    for rozmiar, metryki in wyniki.items():\n",
    "        print(f\"{rozmiar:7d} | {metryki['final_train_acc']:9.4f} | {metryki['test_accuracy']:9.4f} | {metryki['final_loss']:10.4f}\")\n",
    "    \n",
    "    # Znajdź najlepszy rozmiar\n",
    "    najlepszy = max(wyniki.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    print(f\"\\n🏆 NAJLEPSZY: {najlepszy[0]} neuronów (Test Acc: {najlepszy[1]['test_accuracy']:.4f})\")\n",
    "    \n",
    "    return wyniki\n",
    "\n",
    "def trzy_warstwy():\n",
    "    \"\"\"Eksperyment 2: Dodanie trzeciej warstwy\"\"\"\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]  # Etykiety, nie one-hot\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EKSPERYMENT 2: TRZY WARSTWY UKRYTE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Architektura: 784 → 128 → 64 → 10\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "    siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "    \n",
    "    losses, accuracies = siec.fit(X_train_subset, y_train_subset, \n",
    "                                X_val_subset, y_val_subset, epoki=30)\n",
    "    \n",
    "    test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "    \n",
    "    print(f\"\\n⟹ WYNIK: Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "def pokaz_przyklady(siec, X_test, y_test, ile=5):\n",
    "    \"\"\"Pokazuje przykładowe predykcje\"\"\"\n",
    "    print(f\"\\nPrzykładowe predykcje (pierwszych {ile}):\")\n",
    "    print(\"True | Pred | ✓/✗\")\n",
    "    print(\"-\" * 15)\n",
    "    \n",
    "    predictions = siec.predict(X_test[:ile])\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    for i in range(ile):\n",
    "        true_label = y_test[i]\n",
    "        pred_label = predicted_labels[i]\n",
    "        correct = \"✓\" if true_label == pred_label else \"✗\"\n",
    "        print(f\"{true_label:4} | {pred_label:4} | {correct}\")\n",
    "\n",
    "# 5. GŁÓWNY PROGRAM\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - EKSPERYMENTY Z DOKŁADNOŚCIĄ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    print(f\"Dane: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Etykiety - Train: {y_train.shape}, Val: {y_val.shape}, Test: {y_test.shape}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MENU GŁÓWNE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1 - Różne rozmiary warstwy (z dokładnością)\")\n",
    "        print(\"2 - Trzy warstwy ukryte (z dokładnością)\") \n",
    "        print(\"3 - Oba eksperymenty\")\n",
    "        print(\"4 - Pokaż przykładowe predykcje\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            wybor = input(\"Twój wybór (0-4): \").strip()\n",
    "            \n",
    "            if wybor == '1':\n",
    "                eksperyment_rozmiary()\n",
    "            elif wybor == '2':\n",
    "                trzy_warstwy()\n",
    "            elif wybor == '3':\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"URUCHAMIANIE OBIU EKSPERYMENTÓW\")\n",
    "                print(\"=\"*70)\n",
    "                wyniki1 = eksperyment_rozmiary()\n",
    "                wyniki2 = trzy_warstwy()\n",
    "            elif wybor == '4':\n",
    "                # Prosta sieć do pokazania przykładów\n",
    "                siec = SiecNeuronowa()\n",
    "                siec.dodaj_warstwe(Warstwa(784, 64, 0.01))\n",
    "                siec.dodaj_warstwe(WarstwaAktywacji(sigmoid, pochodna_sigmoid))\n",
    "                siec.dodaj_warstwe(Warstwa(64, 10, 0.01))\n",
    "                siec.dodaj_warstwe(WarstwaAktywacji(softmax, pochodna_softmax))\n",
    "                \n",
    "                # Wytrenuj szybko na małych danych\n",
    "                X_temp = X_train[:100]\n",
    "                y_temp = np.zeros((100, 10))\n",
    "                y_temp[np.arange(100), y_train[:100]] = 1\n",
    "                siec.fit(X_temp, y_temp, epoki=5)\n",
    "                \n",
    "                pokaz_przyklady(siec, X_test, y_test, 10)\n",
    "            elif wybor == '0':\n",
    "                print(\"\\nDo widzenia!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Nieprawidłowy wybór.\")\n",
    "                \n",
    "            if wybor != '0':\n",
    "                input(\"\\nNaciśnij Enter aby kontynuować...\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nPrzerwano. Do widzenia!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41b209fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - IMPLEMENTACJA WG OPISU MATEMATYCZNEGO\n",
      "======================================================================\n",
      "Dane: Train: (50000, 784), Test: (10000, 784)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3257, Accuracy: 0.1470\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.1050\n",
      "⟹ Final - Train Acc: 0.1050, Test Acc: 0.1400\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 64 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.2820, Accuracy: 0.1130\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.1160\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.1160\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.1160\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.1160\n",
      "⟹ Final - Train Acc: 0.1160, Test Acc: 0.1400\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 128 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.4731, Accuracy: 0.0740\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.1170\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.1170\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.1170\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.1170\n",
      "⟹ Final - Train Acc: 0.1170, Test Acc: 0.1200\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 256 neuronów\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3023, Accuracy: 0.0960\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0930\n",
      "⟹ Final - Train Acc: 0.0930, Test Acc: 0.0800\n",
      "\n",
      "======================================================================\n",
      "PODSUMOWANIE - RÓŻNE ROZMIARY WARSTWY\n",
      "======================================================================\n",
      "Rozmiar | Train Acc | Test Acc  | Final Loss\n",
      "--------------------------------------------------\n",
      "     32 |    0.1050 |    0.1400 |     0.5000\n",
      "     64 |    0.1160 |    0.1400 |     0.5000\n",
      "    128 |    0.1170 |    0.1200 |     0.5000\n",
      "    256 |    0.0930 |    0.0800 |     0.5000\n",
      "\n",
      "🏆 NAJLEPSZY: 32 neuronów (Test Acc: 0.1400)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "EKSPERYMENT: TRZY WARSTWY UKRYTE\n",
      "======================================================================\n",
      "Architektura: 784 → 128 → 64 → 10\n",
      "Epoka   0, Loss: 1.5336, Accuracy: 0.1210\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0930\n",
      "⟹ Final - Train Acc: 0.0930, Test Acc: 0.0800\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "Do widzenia!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        # Inicjalizacja wag - uwzględniamy bias jako pierwszą kolumnę\n",
    "        self.rozmiar_wejscia = rozmiar_wejscia\n",
    "        self.rozmiar_wyjscia = rozmiar_wyjscia\n",
    "        # Wagi: rozmiar_wyjscia × (rozmiar_wejscia + 1) - +1 dla biasu\n",
    "        self.W = np.random.randn(rozmiar_wyjscia, rozmiar_wejscia + 1) * 0.1\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Pamiętanie danych do propagacji wstecznej\n",
    "        self.x = None  # wektor wejściowy z dodaną jedynką\n",
    "        self.net = None  # net = W · x\n",
    "        self.a = None   # a = φ(net)\n",
    "    \n",
    "    def forward(self, a_prev):\n",
    "        \"\"\"Propagacja w przód - zgodnie z opisem matematycznym\"\"\"\n",
    "        # a_prev to wektor z poprzedniej warstwy (bez biasu)\n",
    "        # Tworzymy x = [1, a_prevᵀ]ᵀ\n",
    "        self.x = np.hstack([np.ones((a_prev.shape[0], 1)), a_prev])\n",
    "        \n",
    "        # net = W · x\n",
    "        self.net = np.dot(self.x, self.W.T)  # W · x\n",
    "        \n",
    "        # a = φ(net)\n",
    "        self.a = self._funkcja_aktywacji(self.net)\n",
    "        \n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, delta_nastepna, W_nastepna=None):\n",
    "        \"\"\"Propagacja w tył - zgodnie z opisem matematycznym\"\"\"\n",
    "        if W_nastepna is not None:\n",
    "            # Warstwa ukryta: δ[l] = (W[l+1]ᵀ · δ[l+1]) ⊙ φ'(net[l])\n",
    "            # Pomijamy bias w wagach następnej warstwy\n",
    "            W_nastepna_bez_biasu = W_nastepna[:, 1:]  # pomijamy wagę dla biasu\n",
    "            delta = np.dot(delta_nastepna, W_nastepna_bez_biasu) * self._pochodna_aktywacji(self.net)\n",
    "        else:\n",
    "            # Warstwa wyjściowa: δ[L] = (a[L] - y) ⊙ φ'(net[L])\n",
    "            delta = delta_nastepna * self._pochodna_aktywacji(self.net)\n",
    "        \n",
    "        # Obliczanie gradientu: ∂L/∂W = δ · xᵀ\n",
    "        gradient_W = np.dot(delta.T, self.x)\n",
    "        \n",
    "        # Aktualizacja wag: W = W - η · ∂L/∂W\n",
    "        self.W -= self.learning_rate * gradient_W\n",
    "        \n",
    "        return delta, self.W\n",
    "    \n",
    "    def _funkcja_aktywacji(self, x):\n",
    "        \"\"\"Funkcja aktywacji - sigmoid\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # clip dla stabilności\n",
    "    \n",
    "    def _pochodna_aktywacji(self, x):\n",
    "        \"\"\"Pochodna funkcji aktywacji\"\"\"\n",
    "        s = self._funkcja_aktywacji(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Propagacja w przód przez wszystkie warstwy\"\"\"\n",
    "        a = X\n",
    "        for warstwa in self.warstwy:\n",
    "            a = warstwa.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"Propagacja w tył - zgodnie z opisem matematycznym\"\"\"\n",
    "        # Ostatnia warstwa: δ[L] = (a[L] - y) ⊙ φ'(net[L])\n",
    "        ostatnia_warstwa = self.warstwy[-1]\n",
    "        delta_L = (ostatnia_warstwa.a - y) * ostatnia_warstwa._pochodna_aktywacji(ostatnia_warstwa.net)\n",
    "        \n",
    "        # Propagacja wstecz przez warstwy\n",
    "        delta = delta_L\n",
    "        W_nastepna = None\n",
    "        \n",
    "        for i in range(len(self.warstwy) - 1, 0, -1):\n",
    "            delta, W_nastepna = self.warstwy[i].backward(delta, W_nastepna)\n",
    "        \n",
    "        # Pierwsza warstwa ukryta\n",
    "        self.warstwy[0].backward(delta, W_nastepna)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10, verbose=True):\n",
    "        \"\"\"Uczenie sieci\"\"\"\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Propagacja w przód\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Obliczanie funkcji kosztu: L = ½∑(y - a[L])²\n",
    "            loss = 0.5 * np.sum((output - y) ** 2) / X.shape[0]\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Obliczanie dokładności\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            true_labels = np.argmax(y, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Propagacja w tył\n",
    "            self.backward(y)\n",
    "            \n",
    "            if verbose and epoka % 10 == 0:\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla nowych danych\"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Oblicza dokładność\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        return np.mean(predicted_labels == y_true)\n",
    "\n",
    "# Funkcje do wczytywania danych (bez zmian)\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set\n",
    "        X_test, y_test = test_set\n",
    "    else:\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set  \n",
    "        X_test, y_test = test_set\n",
    "    \n",
    "    y_train_onehot = np.zeros((y_train.size, 10))\n",
    "    y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# Eksperymenty\n",
    "def eksperyment_rozmiary():\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"Train: {X_train_subset.shape}, Val: {X_val_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    wyniki = {}\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TEST: Warstwa ukryta {rozmiar} neuronów\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.1))  # wyższy learning_rate\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.1))\n",
    "        \n",
    "        losses, accuracies = siec.fit(X_train_subset, y_train_subset, epoki=50, verbose=False)\n",
    "        \n",
    "        # Wypisz postęp co 10 epok\n",
    "        for epoka in range(0, 50, 10):\n",
    "            if epoka < len(losses):\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {losses[epoka]:.4f}, Accuracy: {accuracies[epoka]:.4f}\")\n",
    "        \n",
    "        test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "        \n",
    "        wyniki[rozmiar] = {\n",
    "            'final_loss': losses[-1],\n",
    "            'final_train_acc': accuracies[-1],\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Podsumowanie\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PODSUMOWANIE - RÓŻNE ROZMIARY WARSTWY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Rozmiar | Train Acc | Test Acc  | Final Loss\")\n",
    "    print(\"-\" * 50)\n",
    "    for rozmiar, metryki in wyniki.items():\n",
    "        print(f\"{rozmiar:7d} | {metryki['final_train_acc']:9.4f} | {metryki['test_accuracy']:9.4f} | {metryki['final_loss']:10.4f}\")\n",
    "    \n",
    "    najlepszy = max(wyniki.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    print(f\"\\n🏆 NAJLEPSZY: {najlepszy[0]} neuronów (Test Acc: {najlepszy[1]['test_accuracy']:.4f})\")\n",
    "    \n",
    "    return wyniki\n",
    "\n",
    "def trzy_warstwy():\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EKSPERYMENT: TRZY WARSTWY UKRYTE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Architektura: 784 → 128 → 64 → 10\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.1))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.1))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.1))\n",
    "    \n",
    "    losses, accuracies = siec.fit(X_train_subset, y_train_subset, epoki=50, verbose=False)\n",
    "    \n",
    "    # Wypisz postęp\n",
    "    for epoka in range(0, 50, 10):\n",
    "        if epoka < len(losses):\n",
    "            print(f\"Epoka {epoka:3d}, Loss: {losses[epoka]:.4f}, Accuracy: {accuracies[epoka]:.4f}\")\n",
    "    \n",
    "    test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "    print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Główny program\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - IMPLEMENTACJA WG OPISU MATEMATYCZNEGO\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    print(f\"Dane: Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MENU GŁÓWNE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1 - Różne rozmiary warstwy ukrytej\")\n",
    "        print(\"2 - Trzy warstwy ukryte\") \n",
    "        print(\"0 - Wyjście\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        wybor = input(\"Twój wybór (0-2): \").strip()\n",
    "        \n",
    "        if wybor == '1':\n",
    "            eksperyment_rozmiary()\n",
    "        elif wybor == '2':\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '0':\n",
    "            print(\"\\nDo widzenia!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Nieprawidłowy wybór.\")\n",
    "        \n",
    "        if wybor != '0':\n",
    "            input(\"\\nNaciśnij Enter aby kontynuować...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cccbab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIECI NEURONOWE - POPRAWNA INICJALIZACJA WAG N(0, 1/√n)\n",
      "======================================================================\n",
      "TEST INICJALIZACJI WAG\n",
      "======================================================================\n",
      "\n",
      "1. INICJALIZACJA N(0, 0.1) - STARA METODA:\n",
      "Warstwa 1: 784→128, Średnia: -0.0004, Std: 0.0999\n",
      "Warstwa 2: 128→64, Średnia: -0.0007, Std: 0.1009\n",
      "Warstwa 3: 64→10, Średnia: -0.0005, Std: 0.1003\n",
      "\n",
      "2. INICJALIZACJA N(0, 1/√n[l-1]) - NOWA METODA:\n",
      "Warstwa 1: 784→128, Średnia: -0.0000, Std: 0.0357 (1/√784 = 0.0357)\n",
      "Warstwa 2: 128→64, Średnia: -0.0002, Std: 0.0880 (1/√128 = 0.0884)\n",
      "Warstwa 3: 64→10, Średnia: -0.0011, Std: 0.1311 (1/√64 = 0.1250)\n",
      "\n",
      "Dane: Train: (50000, 784), Test: (10000, 784)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3432, Accuracy: 0.0990\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0930\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0930\n",
      "⟹ Final - Train Acc: 0.0930, Test Acc: 0.0800\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 64 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.1893, Accuracy: 0.1110\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0920\n",
      "⟹ Final - Train Acc: 0.0920, Test Acc: 0.1000\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 128 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.4117, Accuracy: 0.0920\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0870\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0870\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0870\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0870\n",
      "⟹ Final - Train Acc: 0.0870, Test Acc: 0.0500\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 256 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.2254, Accuracy: 0.1130\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0970\n",
      "⟹ Final - Train Acc: 0.0970, Test Acc: 0.0850\n",
      "\n",
      "======================================================================\n",
      "PODSUMOWANIE - RÓŻNE ROZMIARY WARSTWY\n",
      "======================================================================\n",
      "Rozmiar | Train Acc | Test Acc  | Final Loss\n",
      "--------------------------------------------------\n",
      "     32 |    0.0930 |    0.0800 |     0.5000\n",
      "     64 |    0.0920 |    0.1000 |     0.5000\n",
      "    128 |    0.0870 |    0.0500 |     0.5000\n",
      "    256 |    0.0970 |    0.0850 |     0.5000\n",
      "\n",
      "🏆 NAJLEPSZY: 64 neuronów (Test Acc: 0.1000)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "EKSPERYMENT: TRZY WARSTWY UKRYTE (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Architektura: 784 → 128 → 64 → 10\n",
      "Epoka   0, Loss: 1.3095, Accuracy: 0.0930\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.1050\n",
      "⟹ Final - Train Acc: 0.1050, Test Acc: 0.1400\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Epoka   0, Loss: 1.2654, Accuracy: 0.1600\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Train: (1000, 784), Val: (200, 784)\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 32 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3352, Accuracy: 0.0970\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0920\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0920\n",
      "⟹ Final - Train Acc: 0.0920, Test Acc: 0.1000\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 64 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3679, Accuracy: 0.1050\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.1050\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.1050\n",
      "⟹ Final - Train Acc: 0.1050, Test Acc: 0.1400\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 128 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.4153, Accuracy: 0.1060\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0970\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0970\n",
      "⟹ Final - Train Acc: 0.0970, Test Acc: 0.0850\n",
      "\n",
      "======================================================================\n",
      "TEST: Warstwa ukryta 256 neuronów (inicjalizacja N(0, 1/√n))\n",
      "======================================================================\n",
      "Epoka   0, Loss: 1.3100, Accuracy: 0.1050\n",
      "Epoka  10, Loss: 0.5000, Accuracy: 0.0940\n",
      "Epoka  20, Loss: 0.5000, Accuracy: 0.0940\n",
      "Epoka  30, Loss: 0.5000, Accuracy: 0.0940\n",
      "Epoka  40, Loss: 0.5000, Accuracy: 0.0940\n",
      "⟹ Final - Train Acc: 0.0940, Test Acc: 0.1000\n",
      "\n",
      "======================================================================\n",
      "PODSUMOWANIE - RÓŻNE ROZMIARY WARSTWY\n",
      "======================================================================\n",
      "Rozmiar | Train Acc | Test Acc  | Final Loss\n",
      "--------------------------------------------------\n",
      "     32 |    0.0920 |    0.1000 |     0.5000\n",
      "     64 |    0.1050 |    0.1400 |     0.5000\n",
      "    128 |    0.0970 |    0.0850 |     0.5000\n",
      "    256 |    0.0940 |    0.1000 |     0.5000\n",
      "\n",
      "🏆 NAJLEPSZY: 64 neuronów (Test Acc: 0.1400)\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Nieprawidłowy wybór.\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "Nieprawidłowy wybór.\n",
      "\n",
      "==================================================\n",
      "MENU GŁÓWNE\n",
      "==================================================\n",
      "1 - Różne rozmiary warstwy ukrytej\n",
      "2 - Trzy warstwy ukryte\n",
      "3 - Test szybki (małe dane)\n",
      "0 - Wyjście\n",
      "==================================================\n",
      "\n",
      "Do widzenia!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, rozmiar_wejscia, rozmiar_wyjscia, learning_rate=0.01):\n",
    "        self.rozmiar_wejscia = rozmiar_wejscia\n",
    "        self.rozmiar_wyjscia = rozmiar_wyjscia\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # INICJALIZACJA WAG Z ROZKŁADU NORMALNEGO N(0, 1/liczba_neuronow)\n",
    "        # Wagi: rozmiar_wyjscia × (rozmiar_wejscia + 1) - +1 dla biasu\n",
    "        std_dev = 1.0 / np.sqrt(rozmiar_wejscia)  # N(0, 1/n[l-1])\n",
    "        self.W = np.random.randn(rozmiar_wyjscia, rozmiar_wejscia + 1) * std_dev\n",
    "        \n",
    "        # Pamiętanie danych do propagacji wstecznej\n",
    "        self.x = None  # wektor wejściowy z dodaną jedynką\n",
    "        self.net = None  # net = W · x\n",
    "        self.a = None   # a = φ(net)\n",
    "    \n",
    "    def forward(self, a_prev):\n",
    "        \"\"\"Propagacja w przód - zgodnie z opisem matematycznym\"\"\"\n",
    "        # a_prev to wektor z poprzedniej warstwy (bez biasu)\n",
    "        # Tworzymy x = [1, a_prevᵀ]ᵀ\n",
    "        self.x = np.hstack([np.ones((a_prev.shape[0], 1)), a_prev])\n",
    "        \n",
    "        # net = W · x\n",
    "        self.net = np.dot(self.x, self.W.T)  # W · x\n",
    "        \n",
    "        # a = φ(net)\n",
    "        self.a = self._funkcja_aktywacji(self.net)\n",
    "        \n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, delta_nastepna, W_nastepna=None):\n",
    "        \"\"\"Propagacja w tył - zgodnie z opisem matematycznym\"\"\"\n",
    "        if W_nastepna is not None:\n",
    "            # Warstwa ukryta: δ[l] = (W[l+1]ᵀ · δ[l+1]) ⊙ φ'(net[l])\n",
    "            # Pomijamy bias w wagach następnej warstwy\n",
    "            W_nastepna_bez_biasu = W_nastepna[:, 1:]  # pomijamy wagę dla biasu\n",
    "            delta = np.dot(delta_nastepna, W_nastepna_bez_biasu) * self._pochodna_aktywacji(self.net)\n",
    "        else:\n",
    "            # Warstwa wyjściowa: δ[L] = (a[L] - y) ⊙ φ'(net[L])\n",
    "            delta = delta_nastepna * self._pochodna_aktywacji(self.net)\n",
    "        \n",
    "        # Obliczanie gradientu: ∂L/∂W = δ · xᵀ\n",
    "        gradient_W = np.dot(delta.T, self.x)\n",
    "        \n",
    "        # Aktualizacja wag: W = W - η · ∂L/∂W\n",
    "        self.W -= self.learning_rate * gradient_W\n",
    "        \n",
    "        return delta, self.W\n",
    "    \n",
    "    def _funkcja_aktywacji(self, x):\n",
    "        \"\"\"Funkcja aktywacji - sigmoid\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))  # clip dla stabilności\n",
    "    \n",
    "    def _pochodna_aktywacji(self, x):\n",
    "        \"\"\"Pochodna funkcji aktywacji\"\"\"\n",
    "        s = self._funkcja_aktywacji(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self):\n",
    "        self.warstwy = []\n",
    "    \n",
    "    def dodaj_warstwe(self, warstwa):\n",
    "        self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Propagacja w przód przez wszystkie warstwy\"\"\"\n",
    "        a = X\n",
    "        for warstwa in self.warstwy:\n",
    "            a = warstwa.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"Propagacja w tył - zgodnie z opisem matematycznym\"\"\"\n",
    "        # Ostatnia warstwa: δ[L] = (a[L] - y) ⊙ φ'(net[L])\n",
    "        ostatnia_warstwa = self.warstwy[-1]\n",
    "        delta_L = (ostatnia_warstwa.a - y) * ostatnia_warstwa._pochodna_aktywacji(ostatnia_warstwa.net)\n",
    "        \n",
    "        # Propagacja wstecz przez warstwy\n",
    "        delta = delta_L\n",
    "        W_nastepna = None\n",
    "        \n",
    "        for i in range(len(self.warstwy) - 1, 0, -1):\n",
    "            delta, W_nastepna = self.warstwy[i].backward(delta, W_nastepna)\n",
    "        \n",
    "        # Pierwsza warstwa ukryta\n",
    "        self.warstwy[0].backward(delta, W_nastepna)\n",
    "    \n",
    "    def fit(self, X, y, epoki=10, verbose=True):\n",
    "        \"\"\"Uczenie sieci\"\"\"\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Propagacja w przód\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Obliczanie funkcji kosztu: L = ½∑(y - a[L])²\n",
    "            loss = 0.5 * np.sum((output - y) ** 2) / X.shape[0]\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Obliczanie dokładności\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            true_labels = np.argmax(y, axis=1)\n",
    "            accuracy = np.mean(predictions == true_labels)\n",
    "            accuracies.append(accuracy)\n",
    "            \n",
    "            # Propagacja w tył\n",
    "            self.backward(y)\n",
    "            \n",
    "            if verbose and epoka % 10 == 0:\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predykcja dla nowych danych\"\"\"\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        \"\"\"Oblicza dokładność\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        return np.mean(predicted_labels == y_true)\n",
    "\n",
    "# Funkcje do wczytywania danych\n",
    "def wczytaj_mnist(sciezka='mnist.pkl.gz'):\n",
    "    try:\n",
    "        with gzip.open(sciezka, 'rb') as f:\n",
    "            train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "        return train_set, valid_set, test_set\n",
    "    except FileNotFoundError:\n",
    "        print(\"Plik MNIST nie znaleziony. Tworzę tymczasowe dane...\")\n",
    "        return stworz_tymczasowe_dane()\n",
    "\n",
    "def stworz_tymczasowe_dane():\n",
    "    np.random.seed(42)\n",
    "    X_train = np.random.rand(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    y_train_onehot = np.zeros((1000, 10))\n",
    "    y_train_onehot[np.arange(1000), y_train] = 1\n",
    "    \n",
    "    X_test = np.random.rand(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), (X_test, y_test)\n",
    "\n",
    "def przygotuj_dane():\n",
    "    train_set, valid_set, test_set = wczytaj_mnist()\n",
    "    \n",
    "    if len(train_set) == 2:\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set\n",
    "        X_test, y_test = test_set\n",
    "    else:\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = valid_set  \n",
    "        X_test, y_test = test_set\n",
    "    \n",
    "    y_train_onehot = np.zeros((y_train.size, 10))\n",
    "    y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "    \n",
    "    return (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "# Eksperymenty z różnymi inicjalizacjami\n",
    "def test_inicjalizacji():\n",
    "    \"\"\"Test różnych metod inicjalizacji wag\"\"\"\n",
    "    print(\"TEST INICJALIZACJI WAG\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    rozmiary = [784, 128, 64, 10]\n",
    "    \n",
    "    print(\"\\n1. INICJALIZACJA N(0, 0.1) - STARA METODA:\")\n",
    "    for i in range(len(rozmiary)-1):\n",
    "        rozmiar_wejscia = rozmiary[i]\n",
    "        rozmiar_wyjscia = rozmiary[i+1]\n",
    "        wagi = np.random.randn(rozmiar_wyjscia, rozmiar_wejscia + 1) * 0.1\n",
    "        srednia = np.mean(wagi)\n",
    "        std = np.std(wagi)\n",
    "        print(f\"Warstwa {i+1}: {rozmiar_wejscia}→{rozmiar_wyjscia}, Średnia: {srednia:.4f}, Std: {std:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. INICJALIZACJA N(0, 1/√n[l-1]) - NOWA METODA:\")\n",
    "    for i in range(len(rozmiary)-1):\n",
    "        rozmiar_wejscia = rozmiary[i]\n",
    "        rozmiar_wyjscia = rozmiary[i+1]\n",
    "        std_dev = 1.0 / np.sqrt(rozmiar_wejscia)\n",
    "        wagi = np.random.randn(rozmiar_wyjscia, rozmiar_wejscia + 1) * std_dev\n",
    "        srednia = np.mean(wagi)\n",
    "        std = np.std(wagi)\n",
    "        print(f\"Warstwa {i+1}: {rozmiar_wejscia}→{rozmiar_wyjscia}, Średnia: {srednia:.4f}, Std: {std:.4f} (1/√{rozmiar_wejscia} = {std_dev:.4f})\")\n",
    "\n",
    "def eksperyment_rozmiary():\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    X_val_subset = X_val[:200]\n",
    "    y_val_subset = y_val[:200]\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"Train: {X_train_subset.shape}, Val: {X_val_subset.shape}\")\n",
    "    \n",
    "    rozmiary = [32, 64, 128, 256]\n",
    "    wyniki = {}\n",
    "    \n",
    "    for rozmiar in rozmiary:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TEST: Warstwa ukryta {rozmiar} neuronów (inicjalizacja N(0, 1/√n))\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        siec = SiecNeuronowa()\n",
    "        siec.dodaj_warstwe(Warstwa(784, rozmiar, 0.1))\n",
    "        siec.dodaj_warstwe(Warstwa(rozmiar, 10, 0.1))\n",
    "        \n",
    "        losses, accuracies = siec.fit(X_train_subset, y_train_subset, epoki=50, verbose=False)\n",
    "        \n",
    "        # Wypisz postęp co 10 epok\n",
    "        for epoka in range(0, 50, 10):\n",
    "            if epoka < len(losses):\n",
    "                print(f\"Epoka {epoka:3d}, Loss: {losses[epoka]:.4f}, Accuracy: {accuracies[epoka]:.4f}\")\n",
    "        \n",
    "        test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "        \n",
    "        wyniki[rozmiar] = {\n",
    "            'final_loss': losses[-1],\n",
    "            'final_train_acc': accuracies[-1],\n",
    "            'test_accuracy': test_accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Podsumowanie\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PODSUMOWANIE - RÓŻNE ROZMIARY WARSTWY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Rozmiar | Train Acc | Test Acc  | Final Loss\")\n",
    "    print(\"-\" * 50)\n",
    "    for rozmiar, metryki in wyniki.items():\n",
    "        print(f\"{rozmiar:7d} | {metryki['final_train_acc']:9.4f} | {metryki['test_accuracy']:9.4f} | {metryki['final_loss']:10.4f}\")\n",
    "    \n",
    "    najlepszy = max(wyniki.items(), key=lambda x: x[1]['test_accuracy'])\n",
    "    print(f\"\\n🏆 NAJLEPSZY: {najlepszy[0]} neuronów (Test Acc: {najlepszy[1]['test_accuracy']:.4f})\")\n",
    "    \n",
    "    return wyniki\n",
    "\n",
    "def trzy_warstwy():\n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    \n",
    "    X_train_subset = X_train[:1000]\n",
    "    y_train_subset = y_train_onehot[:1000]\n",
    "    \n",
    "    X_test_subset = X_test[:200]\n",
    "    y_test_subset = y_test[:200]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EKSPERYMENT: TRZY WARSTWY UKRYTE (inicjalizacja N(0, 1/√n))\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"Architektura: 784 → 128 → 64 → 10\")\n",
    "    \n",
    "    siec = SiecNeuronowa()\n",
    "    siec.dodaj_warstwe(Warstwa(784, 128, 0.1))\n",
    "    siec.dodaj_warstwe(Warstwa(128, 64, 0.1))\n",
    "    siec.dodaj_warstwe(Warstwa(64, 10, 0.1))\n",
    "    \n",
    "    losses, accuracies = siec.fit(X_train_subset, y_train_subset, epoki=50, verbose=False)\n",
    "    \n",
    "    # Wypisz postęp\n",
    "    for epoka in range(0, 50, 10):\n",
    "        if epoka < len(losses):\n",
    "            print(f\"Epoka {epoka:3d}, Loss: {losses[epoka]:.4f}, Accuracy: {accuracies[epoka]:.4f}\")\n",
    "    \n",
    "    test_accuracy = siec.accuracy(X_test_subset, y_test_subset)\n",
    "    print(f\"⟹ Final - Train Acc: {accuracies[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "# Główny program\n",
    "def main():\n",
    "    print(\"SIECI NEURONOWE - POPRAWNA INICJALIZACJA WAG N(0, 1/√n)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test różnych inicjalizacji\n",
    "    test_inicjalizacji()\n",
    "    \n",
    "    (X_train, y_train_onehot, y_train), (X_val, y_val), (X_test, y_test) = przygotuj_dane()\n",
    "    print(f\"\\nDane: Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MENU GŁÓWNE\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"1 - Różne rozmiary warstwy ukrytej\")\n",
    "        print(\"2 - Trzy warstwy ukryte\") \n",
    "        print(\"3 - Test szybki (małe dane)\")\n",
    "        print(\"0 - Wyjście\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        wybor = input(\"Twój wybór (0-3): \").strip()\n",
    "        \n",
    "        if wybor == '1':\n",
    "            eksperyment_rozmiary()\n",
    "        elif wybor == '2':\n",
    "            trzy_warstwy()\n",
    "        elif wybor == '3':\n",
    "            # Szybki test\n",
    "            X_temp = np.random.rand(50, 784)\n",
    "            y_temp = np.zeros((50, 10))\n",
    "            y_labels = np.random.randint(0, 10, 50)\n",
    "            y_temp[np.arange(50), y_labels] = 1\n",
    "            \n",
    "            siec = SiecNeuronowa()\n",
    "            siec.dodaj_warstwe(Warstwa(784, 32, 0.1))\n",
    "            siec.dodaj_warstwe(Warstwa(32, 10, 0.1))\n",
    "            siec.fit(X_temp, y_temp, epoki=10, verbose=True)\n",
    "        elif wybor == '0':\n",
    "            print(\"\\nDo widzenia!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Nieprawidłowy wybór.\")\n",
    "        \n",
    "        if wybor != '0':\n",
    "            input(\"\\nNaciśnij Enter aby kontynuować...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e4d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
