{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff78f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy funkcje aktywacji fi (sigmoida)\n",
    "# fi' = fi*(1-fi) \n",
    "\n",
    "def sigmoid(x): return (1+np.exp(-x))**(-1) \n",
    "\n",
    "def deriv_sigmoid(x): return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ad269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generujemy wagi\n",
    "def wygeneruj_wagi(wejscie, wyjscie):\n",
    "    # wejsciem jest liczba neuronow w obecnej warstwie\n",
    "    # wyjściem jest liczba neuronów w następnej warstwie\n",
    "    wektor_wag = np.random.normal(0,1/math.sqrt(len(wejscie)),(1+len(wejscie))*wyjscie)\n",
    "    return np.reshape(wektor_wag, (wyjscie, 1+len(wejscie) )  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3f3320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35723577,  1.21747645, -1.22620051],\n",
       "       [-1.25128079,  0.12543397,  0.34059919]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wygeneruj_wagi((1,2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c348c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wejscie = wektor 784\n",
    "# Wyjscie = wektor 10\n",
    "\n",
    "# nerunony_na_warstwe: podajemy liczbę na warstwy głębokie\n",
    "def forward_prop(wejscie_dane, liczb_neuronow_wyjscie ):\n",
    "    \"\"\"Podajemy dane na wejście, dodajemy na bias = 1 na początek wektora (otrzymujemy w ten sposób X).\n",
    "    Zadajemy najpierw ile chcemy mieć neuronów na wyjściu\n",
    "    Nastepnie liczymy net = W*X <- wyjście netto.\n",
    "    Póżniej nakładamy funkcję aktywacji fi na net, otrzymując fi(net) = a <- wyjście z warstwy.\n",
    "    Musimy też zapisywać stany x, net, a dla każdej warstwy (np. lista krotek)\"\"\"\n",
    "\n",
    "    stany_x_net_a = []\n",
    "    # Musimy przerobić X na wektor kolumnowy\n",
    "    X = np.vstack([1,wejscie_dane.reshape(-1,1)])\n",
    "    W = wygeneruj_wagi(wejscie_dane, liczb_neuronow_wyjscie)\n",
    "    net = W @ X\n",
    "    a = sigmoid(net)\n",
    "    stany_x_net_a.append((X,net,a))\n",
    "    return stany_x_net_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e885679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17980467]\n",
      " [0.30104507]]\n",
      "[[0.75236525]\n",
      " [0.45737023]]\n"
     ]
    }
   ],
   "source": [
    "A1 = forward_prop(np.array([1,1,0]), liczb_neuronow_wyjscie=2)[0][2]\n",
    "print(A1)\n",
    "\n",
    "A2 = forward_prop(A1, liczb_neuronow_wyjscie=2)[0][2]\n",
    "print(A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c245355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy propagację wstecz\n",
    "\n",
    "# wyjscie oczekiwane - to co chcielibyśmy uzyskać\n",
    "# wyjscie dane - nasz wynik z forward_prop\n",
    "\n",
    "# Funkcja straty to L = 1/2(a[L]-y)^2, czyli pochodna z L to a[L]-y\n",
    "def back_prop(wyjscie_oczekiwane, wyjscie_dane, wagi_warstwy, stala_uczenia = 0.01 ):\n",
    "\n",
    "    # Ostatnia warstwa\n",
    "   # a_ost = \n",
    "    dL_a_wyjscia = wyjscie_dane - wyjscie_oczekiwane\n",
    "\n",
    "    # mamy pochodną dL/da * fi(net)\n",
    "    delta = dL_a_wyjscia * deriv_sigmoid( inv_sigmoid(wyjscie_dane) )\n",
    "    dL_dW = delta * wyjscie_dane.T\n",
    "\n",
    "    # Liczymy dL/dX i usuwamy pierwszy element, otrzymując dL/da niższej warstwy\n",
    "    dL_dX = wagi_warstwy.T @ delta\n",
    "\n",
    "    dL_da_nizsze = dL_dX[1:]\n",
    "\n",
    "    # Aktualizujemy wagi\n",
    "    W_nowe = wagi_warstwy - stala_uczenia*dL_dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aaecbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[0.1,-0.2,0.3], [-0.4,0.5,-0.6]])\n",
    "B = np.array([1,1,0])\n",
    "C= B.reshape(-1,1)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b666cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1,  0.1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2b69d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/site-packages/numpy/core/shape_base.py:368\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arrs \u001b[38;5;129;01mand\u001b[39;00m arrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "np.hstack([1,A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbc586",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36msigmoid\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid\u001b[39m(x): \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[32m1\u001b[39m+\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)**(-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "sigmoid(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc447554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39628975 -0.08171137 -0.00218662 -0.48415335 -0.11996831]\n",
      " [-0.34005076 -0.05461619 -0.3453193   0.02321433  0.56519641]\n",
      " [ 0.07350721  0.65815865  0.68618035  0.14726593  0.78687395]]\n"
     ]
    }
   ],
   "source": [
    "liczba_neuronow = 3\n",
    "liczba_wejsc = 4\n",
    "x= np.random.randn(liczba_neuronow, liczba_wejsc + 1) * 1/np.sqrt(liczba_wejsc)\n",
    "y = np.random.normal(0,1/math.sqrt(liczba_wejsc),(1+liczba_wejsc)*liczba_neuronow)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ede8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Musimy unormować dane\n",
    "def normuj_dane(dane):\n",
    "    unormowane = []\n",
    "    for  x, y in dane:\n",
    "        x_norm = x/255\n",
    "        unormowane.append((x_norm,y))\n",
    "    return unormowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f793c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Szybki test...\n",
      "Epoka 0, Strata: 0.1055, Dokładność: 0.1170\n",
      "Epoka 1, Strata: 0.0901, Dokładność: 0.1170\n",
      "Epoka 2, Strata: 0.0899, Dokładność: 0.1180\n",
      "Epoka 3, Strata: 0.0898, Dokładność: 0.1650\n",
      "Epoka 4, Strata: 0.0898, Dokładność: 0.1850\n",
      "Epoka 5, Strata: 0.0897, Dokładność: 0.1940\n",
      "Epoka 6, Strata: 0.0897, Dokładność: 0.2110\n",
      "Epoka 7, Strata: 0.0896, Dokładność: 0.1930\n",
      "Epoka 8, Strata: 0.0896, Dokładność: 0.2040\n",
      "Epoka 9, Strata: 0.0895, Dokładność: 0.2170\n",
      "Epoka 10, Strata: 0.0895, Dokładność: 0.1340\n",
      "Epoka 11, Strata: 0.0894, Dokładność: 0.2210\n",
      "Epoka 12, Strata: 0.0894, Dokładność: 0.2050\n",
      "Epoka 13, Strata: 0.0893, Dokładność: 0.1520\n",
      "Epoka 14, Strata: 0.0892, Dokładność: 0.1200\n",
      "Epoka 15, Strata: 0.0892, Dokładność: 0.2240\n",
      "Epoka 16, Strata: 0.0891, Dokładność: 0.2230\n",
      "Epoka 17, Strata: 0.0891, Dokładność: 0.2270\n",
      "Epoka 18, Strata: 0.0890, Dokładność: 0.2560\n",
      "Epoka 19, Strata: 0.0889, Dokładność: 0.2380\n",
      "Epoka 20, Strata: 0.0889, Dokładność: 0.2250\n",
      "Epoka 21, Strata: 0.0888, Dokładność: 0.2410\n",
      "Epoka 22, Strata: 0.0887, Dokładność: 0.2700\n",
      "Epoka 23, Strata: 0.0886, Dokładność: 0.2410\n",
      "Epoka 24, Strata: 0.0885, Dokładność: 0.2320\n",
      "Epoka 25, Strata: 0.0884, Dokładność: 0.2410\n",
      "Epoka 26, Strata: 0.0883, Dokładność: 0.2730\n",
      "Epoka 27, Strata: 0.0882, Dokładność: 0.2850\n",
      "Epoka 28, Strata: 0.0881, Dokładność: 0.2840\n",
      "Epoka 29, Strata: 0.0880, Dokładność: 0.2830\n",
      "Epoka 30, Strata: 0.0879, Dokładność: 0.2850\n",
      "Epoka 31, Strata: 0.0878, Dokładność: 0.2830\n",
      "Epoka 32, Strata: 0.0876, Dokładność: 0.2960\n",
      "Epoka 33, Strata: 0.0874, Dokładność: 0.3030\n",
      "Epoka 34, Strata: 0.0873, Dokładność: 0.3120\n",
      "Epoka 35, Strata: 0.0871, Dokładność: 0.3120\n",
      "Epoka 36, Strata: 0.0869, Dokładność: 0.3030\n",
      "Epoka 37, Strata: 0.0867, Dokładność: 0.3240\n",
      "Epoka 38, Strata: 0.0865, Dokładność: 0.3160\n",
      "Epoka 39, Strata: 0.0862, Dokładność: 0.3370\n",
      "Epoka 40, Strata: 0.0859, Dokładność: 0.3250\n",
      "Epoka 41, Strata: 0.0856, Dokładność: 0.3340\n",
      "Epoka 42, Strata: 0.0853, Dokładność: 0.3390\n",
      "Epoka 43, Strata: 0.0850, Dokładność: 0.3240\n",
      "Epoka 44, Strata: 0.0845, Dokładność: 0.3350\n",
      "Epoka 45, Strata: 0.0841, Dokładność: 0.3210\n",
      "Epoka 46, Strata: 0.0836, Dokładność: 0.3350\n",
      "Epoka 47, Strata: 0.0831, Dokładność: 0.3440\n",
      "Epoka 48, Strata: 0.0825, Dokładność: 0.3500\n",
      "Epoka 49, Strata: 0.0820, Dokładność: 0.3520\n",
      "Epoka 50, Strata: 0.0813, Dokładność: 0.3510\n",
      "Epoka 51, Strata: 0.0806, Dokładność: 0.3680\n",
      "Epoka 52, Strata: 0.0799, Dokładność: 0.3580\n",
      "Epoka 53, Strata: 0.0790, Dokładność: 0.3590\n",
      "Epoka 54, Strata: 0.0782, Dokładność: 0.4100\n",
      "Epoka 55, Strata: 0.0774, Dokładność: 0.3710\n",
      "Epoka 56, Strata: 0.0764, Dokładność: 0.3970\n",
      "Epoka 57, Strata: 0.0756, Dokładność: 0.3950\n",
      "Epoka 58, Strata: 0.0746, Dokładność: 0.4410\n",
      "Epoka 59, Strata: 0.0737, Dokładność: 0.4510\n",
      "Epoka 60, Strata: 0.0728, Dokładność: 0.4300\n",
      "Epoka 61, Strata: 0.0719, Dokładność: 0.5010\n",
      "Epoka 62, Strata: 0.0710, Dokładność: 0.4810\n",
      "Epoka 63, Strata: 0.0702, Dokładność: 0.5180\n",
      "Epoka 64, Strata: 0.0693, Dokładność: 0.5330\n",
      "Epoka 65, Strata: 0.0685, Dokładność: 0.5550\n",
      "Epoka 66, Strata: 0.0676, Dokładność: 0.5520\n",
      "Epoka 67, Strata: 0.0668, Dokładność: 0.5680\n",
      "Epoka 68, Strata: 0.0660, Dokładność: 0.5640\n",
      "Epoka 69, Strata: 0.0652, Dokładność: 0.5840\n",
      "Epoka 70, Strata: 0.0644, Dokładność: 0.5760\n",
      "Epoka 71, Strata: 0.0636, Dokładność: 0.5910\n",
      "Epoka 72, Strata: 0.0629, Dokładność: 0.5830\n",
      "Epoka 73, Strata: 0.0621, Dokładność: 0.5970\n",
      "Epoka 74, Strata: 0.0614, Dokładność: 0.5880\n",
      "Epoka 75, Strata: 0.0607, Dokładność: 0.6110\n",
      "Epoka 76, Strata: 0.0600, Dokładność: 0.6000\n",
      "Epoka 77, Strata: 0.0593, Dokładność: 0.6250\n",
      "Epoka 78, Strata: 0.0586, Dokładność: 0.6120\n",
      "Epoka 79, Strata: 0.0579, Dokładność: 0.6260\n",
      "Epoka 80, Strata: 0.0572, Dokładność: 0.6380\n",
      "Epoka 81, Strata: 0.0566, Dokładność: 0.6400\n",
      "Epoka 82, Strata: 0.0560, Dokładność: 0.6520\n",
      "Epoka 83, Strata: 0.0554, Dokładność: 0.6610\n",
      "Epoka 84, Strata: 0.0548, Dokładność: 0.6670\n",
      "Epoka 85, Strata: 0.0542, Dokładność: 0.6790\n",
      "Epoka 86, Strata: 0.0536, Dokładność: 0.6720\n",
      "Epoka 87, Strata: 0.0530, Dokładność: 0.6810\n",
      "Epoka 88, Strata: 0.0525, Dokładność: 0.6880\n",
      "Epoka 89, Strata: 0.0519, Dokładność: 0.6960\n",
      "Epoka 90, Strata: 0.0513, Dokładność: 0.6770\n",
      "Epoka 91, Strata: 0.0509, Dokładność: 0.6990\n",
      "Epoka 92, Strata: 0.0504, Dokładność: 0.6940\n",
      "Epoka 93, Strata: 0.0498, Dokładność: 0.7140\n",
      "Epoka 94, Strata: 0.0493, Dokładność: 0.7240\n",
      "Epoka 95, Strata: 0.0488, Dokładność: 0.7250\n",
      "Epoka 96, Strata: 0.0483, Dokładność: 0.7240\n",
      "Epoka 97, Strata: 0.0478, Dokładność: 0.7330\n",
      "Epoka 98, Strata: 0.0473, Dokładność: 0.7340\n",
      "Epoka 99, Strata: 0.0468, Dokładność: 0.7440\n",
      "Dokładność na 100 przykładach testowych: 0.0900\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "from mnist_loader import load_data_wrapper\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejsc, liczba_neuronow, funkcja_aktywacji='sigmoid'):\n",
    "        self.liczba_wejsc = liczba_wejsc\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "        \n",
    "        # Inicjalizacja wag\n",
    "        #self.wagi = np.random.randn(liczba_neuronow, liczba_wejsc + 1) * 1/(np.sqrt(liczba_wejsc))\n",
    "        self.wagi = np.random.normal(0,1/math.sqrt(liczba_wejsc),(1+liczba_wejsc)*liczba_neuronow)\n",
    "        self.wagi = np.reshape(self.wagi, (liczba_neuronow, 1+liczba_wejsc )  )\n",
    "        #self.wagi = np.random.randn(liczba_neuronow, liczba_wejsc + 1) * (2/(liczba_wejsc + liczba_neuronow))\n",
    "\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.wejscie = None\n",
    "        self.net = None\n",
    "        self.wyjscie = None\n",
    "        \n",
    "    def aktywacja(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def pochodna_aktywacji(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            s = self.aktywacja(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return np.ones_like(x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X może być pojedynczym wektorem (784,) lub batch (batch_size, 784)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        # Dodajemy bias\n",
    "        X_z_biasem = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        \n",
    "        self.wejscie = X_z_biasem\n",
    "        self.net = self.wejscie @ self.wagi.T\n",
    "        self.wyjscie = self.aktywacja(self.net)\n",
    "        \n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, delta_nastepna, stala_uczenia):\n",
    "        delta = delta_nastepna * self.pochodna_aktywacji(self.net)\n",
    "        \n",
    "        # Oblicz gradienty wag\n",
    "        dW = delta.T @ self.wejscie / self.wejscie.shape[0]\n",
    "        \n",
    "        # Aktualizuj wagi\n",
    "        self.wagi -= stala_uczenia * dW\n",
    "        \n",
    "        # Oblicz błąd do przekazania do poprzedniej warstwy (bez biasu)\n",
    "        delta_prev = delta @ self.wagi[:, 1:]\n",
    "        \n",
    "        return delta_prev\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self, architektura, funkcje_aktywacji=None):\n",
    "        self.warstwy = []\n",
    "        \n",
    "        if funkcje_aktywacji is None:\n",
    "            # Domyślnie: relu dla warstw ukrytych, sigmoid dla wyjścia\n",
    "            funkcje_aktywacji = ['relu'] * (len(architektura) - 2) + ['sigmoid']\n",
    "        \n",
    "        for i in range(len(architektura) - 1):\n",
    "            warstwa = Warstwa(architektura[i], architektura[i+1], funkcje_aktywacji[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        wyjscie = X\n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward(wyjscie)\n",
    "        return wyjscie\n",
    "    \n",
    "    def backward(self, y, stala_uczenia):\n",
    "        # Oblicz błąd na wyjściu\n",
    "        wyjscie_ost = self.warstwy[-1].wyjscie\n",
    "        delta = wyjscie_ost - y\n",
    "        \n",
    "        # Propagacja wsteczna przez wszystkie warstwy\n",
    "        for i in range(len(self.warstwy) - 1, -1, -1):\n",
    "            delta = self.warstwy[i].backward(delta, stala_uczenia)\n",
    "    \n",
    "    def fit(self, training_data, epoki=10, stala_uczenia=0.1, rozmiar_batcha=32, verbose=True):\n",
    "        \"\"\"Trenowanie sieci na danych z MNIST loader\"\"\"\n",
    "        historia_straty = []\n",
    "        \n",
    "        # Konwersja training_data z zip do listy\n",
    "        training_data = list(training_data)\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            # Tasowanie danych\n",
    "            np.random.shuffle(training_data)\n",
    "            \n",
    "            strata_epoki = 0\n",
    "            liczba_batchy = 0\n",
    "            \n",
    "            for i in range(0, len(training_data), rozmiar_batcha):\n",
    "                batch = training_data[i:i + rozmiar_batcha]\n",
    "                \n",
    "                # Przygotowanie batcha\n",
    "                X_batch = np.array([x.reshape(-1) for x, y in batch])\n",
    "                y_batch = np.array([y.reshape(-1) for x, y in batch])\n",
    "                \n",
    "                # Forward propagation\n",
    "                wyjscie = self.forward(X_batch)\n",
    "                \n",
    "                # Oblicz stratę (mean squared error)\n",
    "                strata = np.mean((wyjscie - y_batch) ** 2)\n",
    "                strata_epoki += strata\n",
    "                liczba_batchy += 1\n",
    "                \n",
    "                # Backward propagation\n",
    "                self.backward(y_batch, stala_uczenia)\n",
    "            \n",
    "            strata_epoki /= liczba_batchy\n",
    "            historia_straty.append(strata_epoki)\n",
    "            \n",
    "            if verbose:\n",
    "                # Oblicz dokładność na zbiorze treningowym\n",
    "                dokladnosc = self.accuracy(training_data[:1000])  # Tylko próbka dla szybkości\n",
    "                print(f\"Epoka {epoka}, Strata: {strata_epoki:.4f}, Dokładność: {dokladnosc:.4f}\")\n",
    "        \n",
    "        return historia_straty\n",
    "    \n",
    "    def predict(self, X):\n",
    "        wyjscie = self.forward(X)\n",
    "        return np.argmax(wyjscie, axis=1)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        \"\"\"Oblicza dokładność na danych oblicza dokladnosc na danych testiwych\"\"\"\n",
    "        poprawne = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in data:\n",
    "            # Konwersja do formatu batch\n",
    "            x_batch = x.reshape(1, -1)\n",
    "            y_true = np.argmax(y)\n",
    "            \n",
    "            przewidywanie = self.predict(x_batch)[0]\n",
    "            \n",
    "            if przewidywanie == y_true:\n",
    "                poprawne += 1\n",
    "            total += 1\n",
    "        \n",
    "        return poprawne / total\n",
    "\n",
    "\n",
    "# Prosty test na małym podzbiorze\n",
    "def szybki_test():\n",
    "    print(\"Szybki test...\")\n",
    "    training_data, validation_data, test_data = load_data_wrapper()\n",
    "    training_data = list(training_data)\n",
    "    test_data = list(test_data)\n",
    "\n",
    "    #training_data = normuj_dane(training_data)\n",
    "    #test_data = normuj_dane(test_data)\n",
    "    \n",
    "    # Mała sieć dla szybkiego testu\n",
    "    siec = SiecNeuronowa([784, 128, 64, 10], ['sigmoid', 'sigmoid', 'sigmoid'])\n",
    "    \n",
    "    # Trenowanie na mniejszym zbiorze\n",
    "    mini_training = training_data[:1000]\n",
    "    siec.fit(mini_training, epoki=100, stala_uczenia=0.1, rozmiar_batcha=20)\n",
    "    \n",
    "    # Test na podzbiorze testowych\n",
    "    dokladnosc = siec.accuracy(test_data[:100])\n",
    "    print(f\"Dokładność na {len(test_data[:100])} przykładach testowych: {dokladnosc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    szybki_test()  # Szybki test\n",
    "    # eksperymentuj()  # Pełne eksperymenty\n",
    "\n",
    "    # Stała uczenia 0.1 jest za czasami duża"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ead94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23466bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do eksperymentów\n",
    "def eksperymentuj():\n",
    "    print(\"Ładowanie danych MNIST...\")\n",
    "    training_data, validation_data, test_data = load_data_wrapper()\n",
    "    \n",
    "    # Konwersja do list (ważne dla Python 3)\n",
    "    training_data = list(training_data)\n",
    "    validation_data = list(validation_data)\n",
    "    test_data = list(test_data)\n",
    "    \n",
    "    print(f\"Liczba przykładów treningowych: {len(training_data)}\")\n",
    "    print(f\"Liczba przykładów testowych: {len(test_data)}\")\n",
    "    \n",
    "    # Różne architektury do przetestowania\n",
    "    architektury = [\n",
    "        [784, 128, 10],           # 1 warstwa ukryta, 128 neuronów\n",
    "        [784, 256, 10],           # 1 warstwa ukryta, 256 neuronów  \n",
    "        [784, 64, 64, 10],        # 2 warstwy ukryte, 64 neurony każda\n",
    "        [784, 128, 64, 10],       # 2 warstwy ukryte, różne rozmiary\n",
    "    ]\n",
    "    \n",
    "    for i, architektura in enumerate(architektury):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Eksperyment {i+1}: {architektura}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # ReLU dla warstw ukrytych, sigmoid dla wyjścia\n",
    "        funkcje_aktywacji = ['relu'] * (len(architektura) - 2) + ['sigmoid']\n",
    "        \n",
    "        siec = SiecNeuronowa(architektura, funkcje_aktywacji)\n",
    "        \n",
    "        print(\"Trenowanie...\")\n",
    "        historia = siec.fit(training_data, epoki=20, stala_uczenia=0.1, rozmiar_batcha=64)\n",
    "        \n",
    "        # Testowanie na zbiorze testowym\n",
    "        dokladnosc_test = siec.accuracy(test_data[:1000])  # Tylko próbka dla szybkości\n",
    "        dokladnosc_trening = siec.accuracy(training_data[:1000])\n",
    "        \n",
    "        print(f\"\\nWyniki:\")\n",
    "        print(f\"Dokładność na zbiorze treningowym: {dokladnosc_trening:.4f}\")\n",
    "        print(f\"Dokładność na zbiorze testowym: {dokladnosc_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49f1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Szybki test...\n",
      "Trenowanie...\n",
      "Epoka 0, Strata: 2.3283, Train: 0.1020, Val: 0.0000\n",
      "Epoka 5, Strata: 2.3041, Train: 0.1040, Val: 0.0000\n",
      "Epoka 10, Strata: 2.3004, Train: 0.0860, Val: 0.0000\n",
      "Epoka 15, Strata: 2.2997, Train: 0.1020, Val: 0.0000\n",
      "Epoka 20, Strata: 2.3000, Train: 0.1260, Val: 0.0000\n",
      "Epoka 25, Strata: 2.2991, Train: 0.1080, Val: 0.0000\n",
      "\n",
      "Finalne testowanie...\n",
      "Dokładność na zbiorze treningowym: 0.1160\n",
      "Dokładność na zbiorze testowym: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist_loader import load_data_wrapper\n",
    "\n",
    "class Warstwa:\n",
    "    def __init__(self, liczba_wejsc, liczba_neuronow, funkcja_aktywacji='sigmoid'):\n",
    "        self.liczba_wejsc = liczba_wejsc\n",
    "        self.liczba_neuronow = liczba_neuronow\n",
    "        self.wagi = np.random.randn(liczba_neuronow, liczba_wejsc + 1) * np.sqrt(2.0 / (liczba_wejsc + liczba_neuronow))\n",
    "        self.funkcja_aktywacji = funkcja_aktywacji\n",
    "        self.wejscie = None\n",
    "        self.net = None\n",
    "        self.wyjscie = None\n",
    "        \n",
    "    def aktywacja(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            x = np.clip(x, -20, 20)\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.maximum(0.01 * x, x)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.funkcja_aktywacji == 'softmax':\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def pochodna_aktywacji(self, x):\n",
    "        if self.funkcja_aktywacji == 'sigmoid':\n",
    "            s = self.aktywacja(x)\n",
    "            return s * (1 - s)\n",
    "        elif self.funkcja_aktywacji == 'relu':\n",
    "            return np.where(x > 0, 1, 0.01)\n",
    "        elif self.funkcja_aktywacji == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif self.funkcja_aktywacji == 'softmax':\n",
    "            return 1.0\n",
    "        else:\n",
    "            return np.ones_like(x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        \n",
    "        X_z_biasem = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        self.wejscie = X_z_biasem\n",
    "        self.net = self.wejscie @ self.wagi.T\n",
    "        self.wyjscie = self.aktywacja(self.net)\n",
    "        return self.wyjscie\n",
    "    \n",
    "    def backward(self, delta_nastepna, stala_uczenia):\n",
    "        delta_nastepna = np.clip(delta_nastepna, -1, 1)\n",
    "        delta = delta_nastepna * self.pochodna_aktywacji(self.net)\n",
    "        delta = np.clip(delta, -1, 1)\n",
    "        \n",
    "        dW = delta.T @ self.wejscie / self.wejscie.shape[0]\n",
    "        dW = np.clip(dW, -0.1, 0.1)\n",
    "        \n",
    "        self.wagi -= stala_uczenia * dW\n",
    "        delta_prev = delta @ self.wagi[:, 1:]\n",
    "        return delta_prev\n",
    "\n",
    "class SiecNeuronowa:\n",
    "    def __init__(self, architektura, funkcje_aktywacji=None):\n",
    "        self.warstwy = []\n",
    "        \n",
    "        if funkcje_aktywacji is None:\n",
    "            funkcje_aktywacji = ['relu'] * (len(architektura) - 2) + ['softmax']\n",
    "        \n",
    "        if len(funkcje_aktywacji) != len(architektura) - 1:\n",
    "            required = len(architektura) - 1\n",
    "            given = len(funkcje_aktywacji)\n",
    "            raise ValueError(f\"Błąd: Potrzebujesz {required} funkcji aktywacji, ale podano {given}\")\n",
    "        \n",
    "        for i in range(len(architektura) - 1):\n",
    "            warstwa = Warstwa(architektura[i], architektura[i+1], funkcje_aktywacji[i])\n",
    "            self.warstwy.append(warstwa)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        wyjscie = X\n",
    "        for warstwa in self.warstwy:\n",
    "            wyjscie = warstwa.forward(wyjscie)\n",
    "        return wyjscie\n",
    "    \n",
    "    def backward(self, y, stala_uczenia):\n",
    "        wyjscie_ost = self.warstwy[-1].wyjscie\n",
    "        \n",
    "        if self.warstwy[-1].funkcja_aktywacji == 'softmax':\n",
    "            delta = wyjscie_ost - y\n",
    "        else:\n",
    "            delta = wyjscie_ost - y\n",
    "            \n",
    "        delta = np.clip(delta, -1, 1)\n",
    "        \n",
    "        for i in range(len(self.warstwy) - 1, -1, -1):\n",
    "            delta = self.warstwy[i].backward(delta, stala_uczenia)\n",
    "    \n",
    "    def compute_loss(self, wyjscie, y):\n",
    "        epsilon = 1e-8\n",
    "        wyjscie = np.clip(wyjscie, epsilon, 1 - epsilon)\n",
    "        return -np.mean(np.sum(y * np.log(wyjscie), axis=1))\n",
    "    \n",
    "    def fit(self, training_data, validation_data=None, epoki=10, stala_uczenia=0.01, rozmiar_batcha=32, verbose=True):\n",
    "        \"\"\"Dodajemy validation_data jako opcjonalny parametr\"\"\"\n",
    "        historia_straty = []\n",
    "        training_data = list(training_data)\n",
    "        \n",
    "        for epoka in range(epoki):\n",
    "            np.random.shuffle(training_data)\n",
    "            \n",
    "            strata_epoki = 0\n",
    "            liczba_batchy = 0\n",
    "            \n",
    "            for i in range(0, len(training_data), rozmiar_batcha):\n",
    "                batch = training_data[i:i + rozmiar_batcha]\n",
    "                \n",
    "                X_batch = np.array([(x / 255.0).reshape(-1) for x, y in batch])\n",
    "                y_batch = np.array([y.reshape(-1) for x, y in batch])\n",
    "                \n",
    "                wyjscie = self.forward(X_batch)\n",
    "                \n",
    "                if self.warstwy[-1].funkcja_aktywacji == 'softmax':\n",
    "                    strata = self.compute_loss(wyjscie, y_batch)\n",
    "                else:\n",
    "                    strata = np.mean((wyjscie - y_batch) ** 2)\n",
    "                    \n",
    "                if np.isnan(strata):\n",
    "                    continue\n",
    "                    \n",
    "                strata_epoki += strata\n",
    "                liczba_batchy += 1\n",
    "                \n",
    "                self.backward(y_batch, stala_uczenia)\n",
    "            \n",
    "            if liczba_batchy > 0:\n",
    "                strata_epoki /= liczba_batchy\n",
    "                historia_straty.append(strata_epoki)\n",
    "            \n",
    "            if verbose and epoka % 5 == 0:\n",
    "                dokladnosc_trening = self.accuracy(training_data[:500])\n",
    "                \n",
    "                # Sprawdzaj validation_data jeśli jest podane\n",
    "                if validation_data is not None:\n",
    "                    dokladnosc_val = self.accuracy(validation_data[:200])\n",
    "                    print(f\"Epoka {epoka}, Strata: {strata_epoki:.4f}, Train: {dokladnosc_trening:.4f}, Val: {dokladnosc_val:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoka {epoka}, Strata: {strata_epoki:.4f}, Train Acc: {dokladnosc_trening:.4f}\")\n",
    "        \n",
    "        return historia_straty\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, np.ndarray):\n",
    "            if np.max(X) > 1.0:\n",
    "                X = X / 255.0\n",
    "        wyjscie = self.forward(X)\n",
    "        return np.argmax(wyjscie, axis=1)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        poprawne = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in data:\n",
    "            x_norm = (x / 255.0).reshape(1, -1)\n",
    "            y_true = np.argmax(y)\n",
    "            \n",
    "            przewidywanie = self.predict(x_norm)[0]\n",
    "            \n",
    "            if przewidywanie == y_true:\n",
    "                poprawne += 1\n",
    "            total += 1\n",
    "        \n",
    "        return poprawne / total\n",
    "\n",
    "def szybki_test():\n",
    "    print(\"Szybki test...\")\n",
    "    training_data, validation_data, test_data = load_data_wrapper()\n",
    "    training_data = list(training_data)\n",
    "    validation_data = list(validation_data)\n",
    "    test_data = list(test_data)\n",
    "    \n",
    "    # Użyj softmax dla lepszej klasyfikacji\n",
    "    siec = SiecNeuronowa([784, 128, 64, 10], ['relu', 'relu', 'softmax'])\n",
    "    \n",
    "    # Trenuj na mniejszym zbiorze z walidacją\n",
    "    mini_training = training_data[:5000]\n",
    "    mini_validation = validation_data[:1000]  # Użyj zbioru walidacyjnego\n",
    "    \n",
    "    print(\"Trenowanie...\")\n",
    "    # Przekaż validation_data do monitorowania overfittingu\n",
    "    siec.fit(mini_training, validation_data=mini_validation, epoki=30, \n",
    "             stala_uczenia=0.01, rozmiar_batcha=64, verbose=True)\n",
    "    \n",
    "    print(\"\\nFinalne testowanie...\")\n",
    "    dokladnosc_test = siec.accuracy(test_data[:5000])\n",
    "    dokladnosc_trening = siec.accuracy(mini_training[:1000])\n",
    "    \n",
    "    print(f\"Dokładność na zbiorze treningowym: {dokladnosc_trening:.4f}\")\n",
    "    print(f\"Dokładność na zbiorze testowym: {dokladnosc_test:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    szybki_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576503d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
